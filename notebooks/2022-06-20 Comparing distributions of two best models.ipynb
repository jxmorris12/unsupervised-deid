{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9546b90-40d2-4bf2-ba3b-33c64e05f1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jxm3/research/deidentification/unsupervised-deidentification')\n",
    "\n",
    "from dataloader import WikipediaDataModule\n",
    "from model import AbstractModel, CoordinateAscentModel\n",
    "from utils import get_profile_embeddings_by_model_key\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from model_cfg import model_paths_dict\n",
    "\n",
    "datasets.utils.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "num_cpus = len(os.sched_getaffinity(0))\n",
    "\n",
    "\n",
    "def get_profile_embeddings(model_key: str):\n",
    "    profile_embeddings = get_profile_embeddings_by_model_key(model_key=model_key)\n",
    "\n",
    "    print(\"concatenating train, val, and test profile embeddings\")\n",
    "    all_profile_embeddings = torch.cat(\n",
    "        (profile_embeddings['test'], profile_embeddings['val'], profile_embeddings['train']), dim=0\n",
    "    )\n",
    "\n",
    "    print(\"all_profile_embeddings:\", all_profile_embeddings.shape)\n",
    "    return all_profile_embeddings\n",
    "\n",
    "\n",
    "def get_output_folder_by_model_key(model_key: str) -> str:\n",
    "    adv_csvs_folder = os.path.normpath(\n",
    "        os.path.join(\n",
    "            os.path.abspath(__file__), os.pardir, os.pardir, 'adv_csvs_full'\n",
    "        )\n",
    "    )\n",
    "    return os.path.join(adv_csvs_folder, model_key)\n",
    "\n",
    "def load_adv_csv(dm: WikipediaDataModule) -> pd.DataFrame:\n",
    "    # Load all the stuff\n",
    "    adv_df = None\n",
    "    for model_name in ['model_3_1', 'model_3_2', 'model_3_3__placeholder', 'model_3_4']:\n",
    "        adv_csvs_folder = os.path.normpath(\n",
    "            os.path.join(\n",
    "                os.getcwd(), os.pardir, 'adv_csvs_full'\n",
    "            )\n",
    "        )\n",
    "        print('adv_csvs_folder', adv_csvs_folder)\n",
    "        csv_filenames = glob.glob(\n",
    "            os.path.join(\n",
    "                adv_csvs_folder,\n",
    "                f'{model_name}/results__b_1__k_1__n_1000.csv'\n",
    "            )\n",
    "        )\n",
    "        print(model_name, csv_filenames)\n",
    "        for filename in csv_filenames:\n",
    "            df = pd.read_csv(filename)\n",
    "            df['model_name'] = re.search(r'adv_csvs_full/(model_\\d.+)/.+.csv', filename).group(1)\n",
    "            df['k'] = re.search(r'adv_csvs_full/.+/.+__k_(\\d+)__.+.csv', filename).group(1)\n",
    "            df['i'] = df.index\n",
    "\n",
    "            df = df[df['result_type'] == 'Successful']\n",
    "\n",
    "            mini_df = df[['perturbed_text', 'model_name', 'i', 'k']]\n",
    "            mini_df = mini_df.iloc[:100]\n",
    "            \n",
    "            if adv_df is None:\n",
    "                adv_df = mini_df\n",
    "            else:\n",
    "                adv_df = pd.concat((adv_df, mini_df), axis=0)\n",
    "    \n",
    "    # Load baseline redacted data\n",
    "    mini_val_dataset = dm.test_dataset[:1000]\n",
    "    ner_df = pd.DataFrame(\n",
    "        columns=['perturbed_text'],\n",
    "        data=mini_val_dataset['document_redact_ner_bert']\n",
    "    )\n",
    "    ner_df['model_name'] = 'named_entity'\n",
    "    ner_df['i'] = ner_df.index\n",
    "        \n",
    "    lex_df = pd.DataFrame(\n",
    "        columns=['perturbed_text'],\n",
    "        data=mini_val_dataset['document_redact_lexical']\n",
    "    )\n",
    "    lex_df['model_name'] = 'lexical'\n",
    "    lex_df['i'] = lex_df.index\n",
    "\n",
    "    # Combine both adversarial and baseline redacted data\n",
    "    baseline_df = pd.concat((lex_df, ner_df), axis=0)\n",
    "    baseline_df['k'] = 0\n",
    "    full_df = pd.concat((adv_df, baseline_df), axis=0)\n",
    "\n",
    "    # Put newlines back\n",
    "    full_df['perturbed_text'] = full_df['perturbed_text'].apply(lambda s: s.replace('<SPLIT>', '\\n'))\n",
    "\n",
    "    # Standardize mask tokens\n",
    "    full_df['perturbed_text'] = full_df['perturbed_text'].apply(lambda s: s.replace('[MASK]', dm.mask_token))\n",
    "    full_df['perturbed_text'] = full_df['perturbed_text'].apply(lambda s: s.replace('<mask>', dm.mask_token))\n",
    "\n",
    "    return full_df\n",
    "\n",
    "\n",
    "def get_adv_predictions(model_key: str):\n",
    "    checkpoint_path = model_paths_dict[model_key]\n",
    "    assert isinstance(checkpoint_path, str), f\"invalid checkpoint_path {checkpoint_path} for {model_key}\"\n",
    "    print(f\"running eval on {model_key} loaded from {checkpoint_path}\")\n",
    "    model = CoordinateAscentModel.load_from_checkpoint(\n",
    "        checkpoint_path\n",
    "    )\n",
    "\n",
    "    print(f\"loading data with {num_cpus} CPUs\")\n",
    "    dm = WikipediaDataModule(\n",
    "        document_model_name_or_path=model.document_model_name_or_path,\n",
    "        profile_model_name_or_path=model.profile_model_name_or_path,\n",
    "        dataset_name='wiki_bio',\n",
    "        dataset_train_split='train[:256]',\n",
    "        dataset_val_split='val[:256]',\n",
    "        dataset_test_split='test[:100%]',\n",
    "        dataset_version='1.2.0',\n",
    "        num_workers=num_cpus,\n",
    "        train_batch_size=256,\n",
    "        eval_batch_size=256,\n",
    "        max_seq_length=128,\n",
    "        sample_spans=False,\n",
    "    )\n",
    "    dm.setup(\"fit\")\n",
    "\n",
    "    all_profile_embeddings = get_profile_embeddings(model_key=model_key).cuda()\n",
    "\n",
    "    model.document_model.eval()\n",
    "    model.document_model.cuda()\n",
    "    model.document_embed.eval()\n",
    "    model.document_embed.cuda()\n",
    "\n",
    "    adv_csv = load_adv_csv(dm=dm)\n",
    "\n",
    "    topk_values = []\n",
    "    topk_idxs = []\n",
    "    batch_size = 256\n",
    "    i = 0\n",
    "    while i < len(adv_csv):\n",
    "        ex = adv_csv.iloc[i:i+batch_size]\n",
    "        test_batch = dm.document_tokenizer.batch_encode_plus(\n",
    "            ex['perturbed_text'].tolist(),\n",
    "            max_length=dm.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        test_batch = {\n",
    "            f'perturbed_text__{k}': v for k,v in test_batch.items()\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            document_embeddings = model.forward_document(batch=test_batch, document_type='perturbed_text')\n",
    "            document_to_profile_logits = document_embeddings @ all_profile_embeddings.T\n",
    "            document_to_profile_probs = document_to_profile_logits.softmax(dim=1)\n",
    "            topk_100 = document_to_profile_probs.topk(100)\n",
    "            topk_values.append(topk_100.values)\n",
    "            topk_idxs.append(topk_100.indices)\n",
    "\n",
    "        i += batch_size\n",
    "    \n",
    "    adv_csv['pred_topk_values'] = torch.cat(topk_values, dim=0).cpu().tolist()\n",
    "    adv_csv['pred_topk_idxs'] = torch.cat(topk_idxs, dim=0).cpu().tolist()\n",
    "    return adv_csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5f7b2b8-b7a4-4d26-940f-51a2bd020494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running attack on model_3_3__placeholder loaded from /home/jxm3/research/deidentification/unsupervised-deidentification/saves/ca__roberta__dropout_0.5_1.0_0.0__e3072__ls0.1/deid-wikibio-4_default/1c9464tp_750/checkpoints/epoch=58-step=134342-idf_total.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model with learning_rate = 0.0001 and patience 6\n",
      "loading data with 8 CPUs\n",
      "Initializing WikipediaDataModule with num_workers = 8 and mask token `<mask>`\n",
      "loading wiki_bio[1.2.0] split train[:256]\n",
      "loading wiki_bio[1.2.0] split val[:256]\n",
      "loading wiki_bio[1.2.0] split test[:100%]\n",
      "                        >> loaded 582659 train embeddings from /home/jxm3/research/deidentification/unsupervised-deidentification/embeddings/profile/model_3_3__placeholder/train.pkl\n",
      ">> loaded 72831 val embeddings from /home/jxm3/research/deidentification/unsupervised-deidentification/embeddings/profile/model_3_3__placeholder/val.pkl\n",
      ">> loaded 72831 test embeddings from /home/jxm3/research/deidentification/unsupervised-deidentification/embeddings/profile/model_3_3__placeholder/test.pkl\n",
      "concatenating train, val, and test profile embeddings\n",
      "all_profile_embeddings: torch.Size([728321, 3072])\n",
      "adv_csvs_folder /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full\n",
      "model_3_1 ['/home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full/model_3_1/results__b_1__k_1__n_1000.csv']\n",
      "adv_csvs_folder /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full\n",
      "model_3_2 ['/home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full/model_3_2/results__b_1__k_1__n_1000.csv']\n",
      "adv_csvs_folder /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full\n",
      "model_3_3__placeholder ['/home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full/model_3_3__placeholder/results__b_1__k_1__n_1000.csv']\n",
      "adv_csvs_folder /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full\n",
      "model_3_4 ['/home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full/model_3_4/results__b_1__k_1__n_1000.csv']\n"
     ]
    }
   ],
   "source": [
    "roberta_roberta_predictions = get_adv_predictions(model_key='model_3_3__placeholder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e07e9325-ef8e-4b68-ac91-990c29888d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running attack on model_3_2 loaded from /home/jxm3/research/deidentification/unsupervised-deidentification/saves/ca__roberta__tapas__dropout_-1.0_1.0_0.0__e3072__ls0.1/deid-wikibio-4_lightning_logs/ojgxa1tf_6/checkpoints/epoch=65-step=150282-idf_total.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model with learning_rate = 0.0001 and patience 6\n",
      "loading data with 8 CPUs\n",
      "Initializing WikipediaDataModule with num_workers = 8 and mask token `<mask>`\n",
      "loading wiki_bio[1.2.0] split train[:256]\n",
      "loading wiki_bio[1.2.0] split val[:256]\n",
      "loading wiki_bio[1.2.0] split test[:100%]\n",
      "                        >> loaded 582659 train embeddings from /home/jxm3/research/deidentification/unsupervised-deidentification/embeddings/profile/model_3_2/train.pkl\n",
      ">> loaded 72831 val embeddings from /home/jxm3/research/deidentification/unsupervised-deidentification/embeddings/profile/model_3_2/val.pkl\n",
      ">> loaded 72831 test embeddings from /home/jxm3/research/deidentification/unsupervised-deidentification/embeddings/profile/model_3_2/test.pkl\n",
      "concatenating train, val, and test profile embeddings\n",
      "all_profile_embeddings: torch.Size([728321, 3072])\n",
      "adv_csvs_folder /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full\n",
      "model_3_1 ['/home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full/model_3_1/results__b_1__k_1__n_1000.csv']\n",
      "adv_csvs_folder /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full\n",
      "model_3_2 ['/home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full/model_3_2/results__b_1__k_1__n_1000.csv']\n",
      "adv_csvs_folder /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full\n",
      "model_3_3__placeholder ['/home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full/model_3_3__placeholder/results__b_1__k_1__n_1000.csv']\n",
      "adv_csvs_folder /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full\n",
      "model_3_4 ['/home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full/model_3_4/results__b_1__k_1__n_1000.csv']\n"
     ]
    }
   ],
   "source": [
    "roberta_tapas_predictions = get_adv_predictions(model_key='model_3_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d54a8ca7-d2af-4a6c-8f88-21b52d27c151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perturbed_text</th>\n",
       "      <th>model_name</th>\n",
       "      <th>i</th>\n",
       "      <th>k</th>\n",
       "      <th>pred_topk_values</th>\n",
       "      <th>pred_topk_idxs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;mask&gt; shenoff &lt;mask&gt; ( born february &lt;mask&gt; ,...</td>\n",
       "      <td>model_3_3__placeholder</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.11052840203046799, 0.05406518280506134, 0.0...</td>\n",
       "      <td>[467718, 158460, 530731, 401411, 685027, 68917...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;mask&gt; &lt;mask&gt; ( born &lt;mask&gt; &lt;mask&gt; &lt;mask&gt; in r...</td>\n",
       "      <td>model_3_3__placeholder</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.5141785740852356, 0.09889905899763107, 0.04...</td>\n",
       "      <td>[39633, 1, 627677, 467415, 168807, 305245, 404...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;mask&gt; &lt;mask&gt; ( born &lt;mask&gt; &lt;mask&gt; &lt;mask&gt; &lt;mas...</td>\n",
       "      <td>model_3_3__placeholder</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.04162168875336647, 0.017952967435121536, 0....</td>\n",
       "      <td>[411731, 469648, 333439, 265113, 178437, 21793...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>john &lt;mask&gt; jack '' &lt;mask&gt; ( 21 february 1869 ...</td>\n",
       "      <td>model_3_3__placeholder</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.18395081162452698, 0.15015146136283875, 0.1...</td>\n",
       "      <td>[382939, 455178, 360496, 613292, 3, 296495, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;mask&gt; &lt;mask&gt; &lt;mask&gt; , ( born 7th july 1979 ) ...</td>\n",
       "      <td>model_3_3__placeholder</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2873842716217041, 0.20317216217517853, 0.14...</td>\n",
       "      <td>[248452, 4, 445867, 331079, 646469, 428111, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>&lt;mask&gt; &lt;mask&gt; hildebert &lt;mask&gt; ( ; born 6 marc...</td>\n",
       "      <td>model_3_3__placeholder</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1861652284860611, 0.17227914929389954, 0.03...</td>\n",
       "      <td>[46645, 97, 649379, 524477, 200807, 658420, 27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>&lt;mask&gt; &lt;mask&gt; ( born &lt;mask&gt; &lt;mask&gt; &lt;mask&gt; ) is...</td>\n",
       "      <td>model_3_3__placeholder</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.07223951816558838, 0.06788275390863419, 0.0...</td>\n",
       "      <td>[292621, 452271, 643379, 637442, 278902, 98, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>&lt;mask&gt; bosisio ( born &lt;mask&gt; &lt;mask&gt; &lt;mask&gt; ) i...</td>\n",
       "      <td>model_3_3__placeholder</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.47312384843826294, 0.4663839340209961, 0.01...</td>\n",
       "      <td>[478090, 99, 131467, 307770, 492295, 345943, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>&lt;mask&gt; &lt;mask&gt; ( born &lt;mask&gt; &lt;mask&gt; , &lt;mask&gt; ) ...</td>\n",
       "      <td>model_3_3__placeholder</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.20568141341209412, 0.11684911698102951, 0.0...</td>\n",
       "      <td>[428736, 610597, 429695, 292739, 512155, 100, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>vice admiral &lt;mask&gt; &lt;mask&gt;. &lt;mask&gt; , united st...</td>\n",
       "      <td>model_3_3__placeholder</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.17401713132858276, 0.12818169593811035, 0.1...</td>\n",
       "      <td>[447251, 43058, 101, 451252, 495942, 662715, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        perturbed_text  \\\n",
       "0    <mask> shenoff <mask> ( born february <mask> ,...   \n",
       "1    <mask> <mask> ( born <mask> <mask> <mask> in r...   \n",
       "2    <mask> <mask> ( born <mask> <mask> <mask> <mas...   \n",
       "3    john <mask> jack '' <mask> ( 21 february 1869 ...   \n",
       "4    <mask> <mask> <mask> , ( born 7th july 1979 ) ...   \n",
       "..                                                 ...   \n",
       "97   <mask> <mask> hildebert <mask> ( ; born 6 marc...   \n",
       "98   <mask> <mask> ( born <mask> <mask> <mask> ) is...   \n",
       "99   <mask> bosisio ( born <mask> <mask> <mask> ) i...   \n",
       "100  <mask> <mask> ( born <mask> <mask> , <mask> ) ...   \n",
       "101  vice admiral <mask> <mask>. <mask> , united st...   \n",
       "\n",
       "                 model_name    i  k  \\\n",
       "0    model_3_3__placeholder    0  1   \n",
       "1    model_3_3__placeholder    1  1   \n",
       "2    model_3_3__placeholder    2  1   \n",
       "3    model_3_3__placeholder    3  1   \n",
       "4    model_3_3__placeholder    4  1   \n",
       "..                      ...  ... ..   \n",
       "97   model_3_3__placeholder   97  1   \n",
       "98   model_3_3__placeholder   98  1   \n",
       "99   model_3_3__placeholder   99  1   \n",
       "100  model_3_3__placeholder  100  1   \n",
       "101  model_3_3__placeholder  101  1   \n",
       "\n",
       "                                      pred_topk_values  \\\n",
       "0    [0.11052840203046799, 0.05406518280506134, 0.0...   \n",
       "1    [0.5141785740852356, 0.09889905899763107, 0.04...   \n",
       "2    [0.04162168875336647, 0.017952967435121536, 0....   \n",
       "3    [0.18395081162452698, 0.15015146136283875, 0.1...   \n",
       "4    [0.2873842716217041, 0.20317216217517853, 0.14...   \n",
       "..                                                 ...   \n",
       "97   [0.1861652284860611, 0.17227914929389954, 0.03...   \n",
       "98   [0.07223951816558838, 0.06788275390863419, 0.0...   \n",
       "99   [0.47312384843826294, 0.4663839340209961, 0.01...   \n",
       "100  [0.20568141341209412, 0.11684911698102951, 0.0...   \n",
       "101  [0.17401713132858276, 0.12818169593811035, 0.1...   \n",
       "\n",
       "                                        pred_topk_idxs  \n",
       "0    [467718, 158460, 530731, 401411, 685027, 68917...  \n",
       "1    [39633, 1, 627677, 467415, 168807, 305245, 404...  \n",
       "2    [411731, 469648, 333439, 265113, 178437, 21793...  \n",
       "3    [382939, 455178, 360496, 613292, 3, 296495, 20...  \n",
       "4    [248452, 4, 445867, 331079, 646469, 428111, 13...  \n",
       "..                                                 ...  \n",
       "97   [46645, 97, 649379, 524477, 200807, 658420, 27...  \n",
       "98   [292621, 452271, 643379, 637442, 278902, 98, 4...  \n",
       "99   [478090, 99, 131467, 307770, 492295, 345943, 4...  \n",
       "100  [428736, 610597, 429695, 292739, 512155, 100, ...  \n",
       "101  [447251, 43058, 101, 451252, 495942, 662715, 2...  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_roberta_predictions[roberta_roberta_predictions['model_name'] == 'model_3_3__placeholder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7b7d17f-76de-4401-94f3-984b87265f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perturbed_text</th>\n",
       "      <th>i</th>\n",
       "      <th>k</th>\n",
       "      <th>pred_topk_values</th>\n",
       "      <th>pred_topk_idxs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lexical</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_3_1</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_3_2</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_3_3__placeholder</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_3_4</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>named_entity</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        perturbed_text     i     k  pred_topk_values  \\\n",
       "model_name                                                             \n",
       "lexical                           1000  1000  1000              1000   \n",
       "model_3_1                          100   100   100               100   \n",
       "model_3_2                          100   100   100               100   \n",
       "model_3_3__placeholder             100   100   100               100   \n",
       "model_3_4                          100   100   100               100   \n",
       "named_entity                      1000  1000  1000              1000   \n",
       "\n",
       "                        pred_topk_idxs  \n",
       "model_name                              \n",
       "lexical                           1000  \n",
       "model_3_1                          100  \n",
       "model_3_2                          100  \n",
       "model_3_3__placeholder             100  \n",
       "model_3_4                          100  \n",
       "named_entity                      1000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_roberta_predictions.groupby('model_name').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78fa57ac-69b7-4eb9-a528-fcd2095c289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_name = {\n",
    "    'model_3_1': 'roberta_tapas__no_masking',\n",
    "    'model_3_2': 'roberta_tapas',\n",
    "    'model_3_3__placeholder': 'roberta_roberta',\n",
    "    'model_3_4': 'pmlm_tapas'\n",
    "}\n",
    "roberta_roberta_predictions['model_name'] = roberta_roberta_predictions['model_name'].apply(lambda s: new_model_name.get(s,s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bbdc140-a487-4352-acc3-13d99eba9fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perturbed_text</th>\n",
       "      <th>model_name</th>\n",
       "      <th>i</th>\n",
       "      <th>k</th>\n",
       "      <th>roberta_roberta__pred_topk_values</th>\n",
       "      <th>roberta_roberta__pred_topk_idxs</th>\n",
       "      <th>roberta_tapas__pred_topk_values</th>\n",
       "      <th>roberta_tapas__pred_topk_idxs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>leonard shenoff &lt;mask&gt; ( born february 12 , &lt;m...</td>\n",
       "      <td>roberta_tapas__no_masking</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.9138374924659729, 0.019291497766971588, 0.0...</td>\n",
       "      <td>[0, 578457, 120349, 267136, 532648, 719788, 49...</td>\n",
       "      <td>[0.8143362402915955, 0.022000230848789215, 0.0...</td>\n",
       "      <td>[0, 5473, 685461, 144197, 267136, 724119, 1798...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;mask&gt; &lt;mask&gt; ( born 25 august &lt;mask&gt; in rhège...</td>\n",
       "      <td>roberta_tapas__no_masking</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.9647166728973389, 0.0008567320764996111, 0....</td>\n",
       "      <td>[1, 100573, 39633, 569950, 176364, 255263, 395...</td>\n",
       "      <td>[0.18824410438537598, 0.14044655859470367, 0.1...</td>\n",
       "      <td>[627677, 467415, 1, 39633, 193135, 646267, 728...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;mask&gt; &lt;mask&gt; ( born 14 june &lt;mask&gt; in dvůr kr...</td>\n",
       "      <td>roberta_tapas__no_masking</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.9932199120521545, 0.0012878905981779099, 0....</td>\n",
       "      <td>[2, 333439, 654853, 507896, 122539, 224290, 50...</td>\n",
       "      <td>[0.5528936982154846, 0.2653157114982605, 0.033...</td>\n",
       "      <td>[2, 456329, 333439, 507309, 534720, 4831, 4452...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>john `` &lt;mask&gt; '' &lt;mask&gt; ( 21 february &lt;mask&gt; ...</td>\n",
       "      <td>roberta_tapas__no_masking</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.7518554925918579, 0.04164363816380501, 0.00...</td>\n",
       "      <td>[3, 455178, 382939, 202957, 360496, 296495, 45...</td>\n",
       "      <td>[0.8678763508796692, 0.027283597737550735, 0.0...</td>\n",
       "      <td>[3, 202957, 382939, 296495, 516160, 34822, 345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>william &lt;mask&gt; &lt;mask&gt; , ( born 7th july 1979 )...</td>\n",
       "      <td>roberta_tapas__no_masking</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.41385799646377563, 0.09564604610204697, 0.0...</td>\n",
       "      <td>[4, 248452, 254701, 391864, 331079, 428111, 44...</td>\n",
       "      <td>[0.8090968728065491, 0.08909827470779419, 0.00...</td>\n",
       "      <td>[4, 520062, 254701, 128242, 445867, 61930, 365...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      perturbed_text  \\\n",
       "0  leonard shenoff <mask> ( born february 12 , <m...   \n",
       "1  <mask> <mask> ( born 25 august <mask> in rhège...   \n",
       "2  <mask> <mask> ( born 14 june <mask> in dvůr kr...   \n",
       "3  john `` <mask> '' <mask> ( 21 february <mask> ...   \n",
       "4  william <mask> <mask> , ( born 7th july 1979 )...   \n",
       "\n",
       "                  model_name  i  k  \\\n",
       "0  roberta_tapas__no_masking  0  1   \n",
       "1  roberta_tapas__no_masking  1  1   \n",
       "2  roberta_tapas__no_masking  2  1   \n",
       "3  roberta_tapas__no_masking  3  1   \n",
       "4  roberta_tapas__no_masking  4  1   \n",
       "\n",
       "                   roberta_roberta__pred_topk_values  \\\n",
       "0  [0.9138374924659729, 0.019291497766971588, 0.0...   \n",
       "1  [0.9647166728973389, 0.0008567320764996111, 0....   \n",
       "2  [0.9932199120521545, 0.0012878905981779099, 0....   \n",
       "3  [0.7518554925918579, 0.04164363816380501, 0.00...   \n",
       "4  [0.41385799646377563, 0.09564604610204697, 0.0...   \n",
       "\n",
       "                     roberta_roberta__pred_topk_idxs  \\\n",
       "0  [0, 578457, 120349, 267136, 532648, 719788, 49...   \n",
       "1  [1, 100573, 39633, 569950, 176364, 255263, 395...   \n",
       "2  [2, 333439, 654853, 507896, 122539, 224290, 50...   \n",
       "3  [3, 455178, 382939, 202957, 360496, 296495, 45...   \n",
       "4  [4, 248452, 254701, 391864, 331079, 428111, 44...   \n",
       "\n",
       "                     roberta_tapas__pred_topk_values  \\\n",
       "0  [0.8143362402915955, 0.022000230848789215, 0.0...   \n",
       "1  [0.18824410438537598, 0.14044655859470367, 0.1...   \n",
       "2  [0.5528936982154846, 0.2653157114982605, 0.033...   \n",
       "3  [0.8678763508796692, 0.027283597737550735, 0.0...   \n",
       "4  [0.8090968728065491, 0.08909827470779419, 0.00...   \n",
       "\n",
       "                       roberta_tapas__pred_topk_idxs  \n",
       "0  [0, 5473, 685461, 144197, 267136, 724119, 1798...  \n",
       "1  [627677, 467415, 1, 39633, 193135, 646267, 728...  \n",
       "2  [2, 456329, 333439, 507309, 534720, 4831, 4452...  \n",
       "3  [3, 202957, 382939, 296495, 516160, 34822, 345...  \n",
       "4  [4, 520062, 254701, 128242, 445867, 61930, 365...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df = roberta_roberta_predictions.rename(columns={'pred_topk_values': 'roberta_roberta__pred_topk_values', 'pred_topk_idxs': 'roberta_roberta__pred_topk_idxs'})\n",
    "out_df['roberta_tapas__pred_topk_values'] = roberta_tapas_predictions['pred_topk_values']\n",
    "out_df['roberta_tapas__pred_topk_idxs'] = roberta_tapas_predictions['pred_topk_idxs']\n",
    "out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7e12121-c8d0-46c8-a409-14c42a2ea248",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df['roberta_roberta__was_correct'] = out_df.apply(lambda row: row['i'] == row['roberta_roberta__pred_topk_idxs'][0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "010f271e-f1ae-492d-b101-e75725e69aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df['roberta_tapas__was_correct'] = out_df.apply(lambda row: row['i'] == row['roberta_tapas__pred_topk_idxs'][0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ecf0aba-f689-478a-a392-c4a3e66b2252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>roberta_roberta__was_correct</th>\n",
       "      <th>roberta_tapas__was_correct</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lexical</th>\n",
       "      <td>499.50</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>named_entity</th>\n",
       "      <td>499.50</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pmlm_tapas</th>\n",
       "      <td>50.37</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta_roberta</th>\n",
       "      <td>50.56</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta_tapas</th>\n",
       "      <td>51.09</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roberta_tapas__no_masking</th>\n",
       "      <td>50.17</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                i  roberta_roberta__was_correct  \\\n",
       "model_name                                                        \n",
       "lexical                    499.50                         0.164   \n",
       "named_entity               499.50                         0.674   \n",
       "pmlm_tapas                  50.37                         0.810   \n",
       "roberta_roberta             50.56                         0.000   \n",
       "roberta_tapas               51.09                         0.770   \n",
       "roberta_tapas__no_masking   50.17                         0.890   \n",
       "\n",
       "                           roberta_tapas__was_correct  \n",
       "model_name                                             \n",
       "lexical                                         0.224  \n",
       "named_entity                                    0.557  \n",
       "pmlm_tapas                                      0.500  \n",
       "roberta_roberta                                 0.510  \n",
       "roberta_tapas                                   0.000  \n",
       "roberta_tapas__no_masking                       0.780  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df.groupby('model_name').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3375e6d0-b012-477c-8f67-61251400a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.to_csv('model_comparison_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4761b6c0-2ad2-400f-9f19-a6515592155d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jxm3/research/deidentification/unsupervised-deidentification/notebooks/model_comparison_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "!realpath model_comparison_predictions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e519c0d3-39d1-4d92-bcd2-740e37beda4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t [     0 157382 202669 453258 647283]\n",
      "1 \t [627677 467415      1  39633 193135]\n",
      "2 \t [456329      2 333439 507309 445282]\n",
      "3 \t [     3 382939 345455 176246 202957]\n",
      "4 \t [     4 520062 254701 128242 445867]\n",
      "5 \t [     5 399211 357441 406264 553956]\n",
      "6 \t [396740 447046 170143 210733 284738]\n",
      "7 \t [     7 395200 191185 320731 367417]\n",
      "8 \t [601929 461665 176324 354936  30302]\n",
      "9 \t [     9 317732 235567 286404  18112]\n",
      "10 \t [    10 711653 708314 148504 408799]\n",
      "11 \t [530409  41750 380123 512320 668322]\n",
      "12 \t [569603 338498     12  19054 285513]\n",
      "13 \t [451541     13 348456 285891 123096]\n",
      "14 \t [    14 229972  19055 398523 323485]\n",
      "15 \t [    15 308063 604486 480493 408304]\n",
      "16 \t [    16 448666 269221 379587 252302]\n",
      "17 \t [383229 571143 374161     17   8379]\n",
      "18 \t [225774 256563 698969 670183     18]\n",
      "19 \t [    19 436791  58874  87961 426805]\n",
      "20 \t [    20 484930 160605 483058 524858]\n",
      "21 \t [441863     21 567151 614879 547715]\n",
      "22 \t [492087 139891 105389 126565 612756]\n",
      "23 \t [    23 384675  18320 653767 111297]\n",
      "24 \t [    24 163061 231608 217913   8541]\n",
      "25 \t [697324 274179 641466  29520 600275]\n",
      "26 \t [644621 636213 169521 420698 610411]\n",
      "27 \t [    27 531138 471327 687488 586235]\n",
      "28 \t [    28 108600 622525 362457 590048]\n",
      "29 \t [155710 233928 655863 312258 156877]\n",
      "30 \t [244951 595256 631899 205344     30]\n",
      "31 \t [    31 524419  87573 464010 219358]\n",
      "32 \t [    32 510154 286327 239791 164523]\n",
      "33 \t [498556 214654 149687 448794 358199]\n",
      "34 \t [419947 185947 514084 571225 271730]\n",
      "35 \t [    36 379521  49661 613555 143196]\n",
      "36 \t [578149     37 644078 594879 139379]\n",
      "37 \t [171915 694457 602922 422826 494809]\n",
      "38 \t [    39 108300 225214  37880  93105]\n",
      "39 \t [ 66601 134685 527533 181958 105960]\n",
      "40 \t [113737     41 424744 400932   6598]\n",
      "41 \t [    42 263059 105874 374562 523268]\n",
      "42 \t [    43 697136 673521 174918 709299]\n",
      "43 \t [    44 396001 195643 529875 167585]\n",
      "44 \t [598434 166518 487973 309570    147]\n",
      "45 \t [419931 456655 292794  52378 346457]\n",
      "46 \t [    47 240551 324377 475990 658023]\n",
      "47 \t [294637 621475 699167 153922 390567]\n",
      "48 \t [656435  38029 683127 443015 134511]\n",
      "49 \t [    50 279055  84665 181051 271959]\n",
      "50 \t [    51 564669  47347  24477 113095]\n",
      "51 \t [    52 510551 528533 351792  34226]\n",
      "52 \t [    53  38568 333462 213419 146674]\n",
      "53 \t [    54 664116 649871  54028  17662]\n",
      "54 \t [242474  70466 651158 284187 451755]\n",
      "55 \t [    56 342280 327204 191322 482765]\n",
      "56 \t [ 61218     57  29758 119933 199586]\n",
      "57 \t [296815 372457 601494 231196     58]\n",
      "58 \t [    59 282025 260880 251230 443358]\n",
      "59 \t [ 52063 434093     60 157598 467192]\n",
      "60 \t [    61 310982 511368 651631 437085]\n",
      "61 \t [344828 217273     62 300561 212631]\n",
      "62 \t [687003 262985 224577     63 258204]\n",
      "63 \t [682547     64 326318  17934 410298]\n",
      "64 \t [313146 227263 577036 126477 100402]\n",
      "65 \t [704148 577941 313027 273461 680514]\n",
      "66 \t [    67 279314 434725 343900  46679]\n",
      "67 \t [471337 346204  86211  92213 146981]\n",
      "68 \t [   319 517369 228648 622378  49750]\n",
      "69 \t [  3321 569002 694003 281997 269048]\n",
      "70 \t [    71 254574 449845 114242 702117]\n",
      "71 \t [355459 683474 267365 340903 152870]\n",
      "72 \t [    73 114033  59615  37636 268787]\n",
      "73 \t [    74 533980 177447 658215 546802]\n",
      "74 \t [    75 191954 551556 287187 550421]\n",
      "75 \t [    76 701735 677344 489466 540768]\n",
      "76 \t [    77 528172 267987 247598 358154]\n",
      "77 \t [188807 499692 222423  20324 640840]\n",
      "78 \t [417397 337478     79 499891 421753]\n",
      "79 \t [219363     80 460506 712570 115715]\n",
      "80 \t [347451 579854 724585 641781 568449]\n",
      "81 \t [    83 525305 334961  53942 553774]\n",
      "82 \t [    84 619890 439349 211563  67617]\n",
      "83 \t [380160 479786     85 674000 181147]\n",
      "84 \t [104409 232699     86 264256 644531]\n",
      "85 \t [105923     87 623877 572529 116027]\n",
      "86 \t [    88 146578 318848 298954 624431]\n",
      "87 \t [    89 559788 276604 198085 184953]\n",
      "88 \t [450926     90 727174  50250 612892]\n",
      "89 \t [278053 512139 388621 658332  61967]\n",
      "90 \t [    92 706633 584398 221038 352587]\n",
      "91 \t [555984     93 352323 705897 381417]\n",
      "92 \t [ 48457  67351 213569  12812 438416]\n",
      "93 \t [    95 371009 179891  18911 508246]\n",
      "94 \t [    96 575100 620818 514547 331073]\n",
      "95 \t [    97 590436 335694 160047 378902]\n",
      "96 \t [118449 726456 474909 263097   8635]\n",
      "97 \t [    99 577830  42149 105164 457905]\n",
      "98 \t [   100 705957 512155 428736 292739]\n",
      "99 \t [   101 131794  43058 451252 380327]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "roberta_tapas__predictions_for__pmlm_tapas = np.vstack(out_df[out_df['model_name'] == 'pmlm_tapas']['roberta_tapas__pred_topk_idxs'])\n",
    "for i in range(len(roberta_tapas__predictions_for__pmlm_tapas)):\n",
    "    print(i,'\\t', roberta_tapas__predictions_for__pmlm_tapas[i,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d8fc705-5f3e-47fb-a482-0fa91383ceb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00525e0fc0d14a468db82e42711859a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "d = datasets.load_dataset(\"wiki_bio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a909c1-115c-475d-9529-d7c07b46d9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
