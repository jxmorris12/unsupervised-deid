{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c8f5b4f-db69-489a-becb-b0455c0b15ff",
   "metadata": {},
   "source": [
    "### BM-25 based redaction\n",
    "\n",
    "We think that the model we have now is good, but it's failing on cases where the hardest words are almost redacted in order of how difficult they are rated from BM25. We want to develop a function that looks like `redact(text: str, p: float)` where `p%` of words are redacted, **in order of importance as measured by BM-25**. This notebook is where I'll figure out how to do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c35b72f-8de5-402f-808a-ec02cc663acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a12cb1b4-021c-4666-a6a6-62f9614894b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_bio (/home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0102069af7eb4719af0fbe1cc0ed93de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/9104 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bab4a4753b54ba4be1d54e666f77cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/9104 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4836e1d1e8534e13bc278ceb6c1548dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/9104 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2390ecd8e0498a8cde6c21813ca8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/9103 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a1d51b098145c4b7dcf17c011ba7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/9104 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a50047f3954c2688055656116fe7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/9104 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45dce8dcac3a4ec0baf0ab00369fe387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/9104 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5a8869b87247b0bbca9dd74a0ddd95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/9104 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing corpi\n",
      "creating search index\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "import datasets\n",
    "import os\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "num_cpus = len(os.sched_getaffinity(0))\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "words_from_text_re = re.compile(r'\\b\\w+\\b')\n",
    "def words_from_text(s: str) -> List[str]:\n",
    "    assert isinstance(s, str)\n",
    "    return words_from_text_re.findall(s)\n",
    "\n",
    "def get_words_from_doc(s: str) -> List[str]:\n",
    "    words = words_from_text(s)\n",
    "    return [w for w in words]\n",
    "\n",
    "split = 'test[:100%]'\n",
    "prof_data = datasets.load_dataset('wiki_bio', split=split, version='1.2.0')\n",
    "\n",
    "def make_table_str(ex):\n",
    "    ex['table_str'] = (\n",
    "        ' '.join(ex['input_text']['table']['column_header'] + ex['input_text']['table']['content'])\n",
    "    )\n",
    "    return ex\n",
    "\n",
    "prof_data = prof_data.map(make_table_str, num_proc=num_cpus)\n",
    "profile_corpus = prof_data['table_str']\n",
    "document_corpus = prof_data['target_text']\n",
    "\n",
    "print(\"tokenizing corpi\")\n",
    "tokenized_document_corpus = [\n",
    "    get_words_from_doc(doc) for doc in document_corpus\n",
    "]\n",
    "tokenized_profile_corpus = [\n",
    "    get_words_from_doc(prof) for prof in profile_corpus\n",
    "]\n",
    "\n",
    "print(\"creating search index\")\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_profile_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf06e94e-5a3e-401d-90ee-3974e8aa8805",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointBM250kapi(BM25Okapi):\n",
    "    \"\"\"A BM250kapi that takes extra documents to calculate idf but only returns scores within initial set of documents.\n",
    "    \n",
    "    This allows us to search only among profiles but use both profiles and documents to calculate inverse document frequency\n",
    "    of terms. That's especially useful since stopwords mostly just appear in documents (and in a small set of profiles with\n",
    "    captions) but they don't provide much utility to the search.\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus, extra_corpus):\n",
    "        super().__init__(corpus + extra_corpus)\n",
    "        self.doc_freqs = self.doc_freqs[:len(corpus)] # truncate extra docs\n",
    "        self.doc_len = self.doc_len[:len(corpus)]\n",
    "        # avgdl = num_doc / self.corpus_size\n",
    "        self.avgdl = self.avgdl * (len(corpus) / (len(corpus) + len(extra_corpus)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1827240-62ae-45d5-bc49-f0b04d4a942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = JointBM250kapi(tokenized_profile_corpus, tokenized_document_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1760a7f-eb29-409f-b851-659556469ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc = document_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc71c09b-c024-4d0f-b738-5f0fa3d50c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shenoff 11.483575607564859\n",
      "randle 8.86848538161152\n",
      "phase 7.927877357931569\n",
      "tenth 6.643360193899222\n",
      "secondary 6.638090149727103\n",
      "leonard 6.035897854707147\n",
      "senators 5.982533239847946\n",
      "pick 5.662013688054766\n",
      "overall 4.400667304180287\n",
      "round 4.202138668427599\n",
      "1949 4.017412641221092\n",
      "baseball 3.9223006894414008\n",
      "draft 3.892086532265137\n",
      "washington 3.875748716013044\n",
      "1970 3.6766192332095002\n",
      "major 3.509989392146517\n",
      "lrb 2.687126848657929\n",
      "rrb 2.687126848657929\n",
      "12 2.606354664662753\n",
      "player 2.5143087395018817\n",
      "former 2.4727241673859144\n",
      "league 2.43310568243545\n",
      "first 2.2890035548959276\n",
      "february 2.183931261454566\n",
      "june 2.080851818385721\n",
      "he 0.9555266540517113\n",
      "was 0.9023216788513828\n",
      "born 0.7554652625737273\n",
      "is 0.5936523046588995\n",
      "a 0.23573687768198504\n",
      "in 0.2263029407585435\n",
      "of 0.17974547640818095\n",
      "the 0.020815920860638215\n"
     ]
    }
   ],
   "source": [
    "sample_doc_words = list(set(words_from_text(sample_doc)))\n",
    "sample_doc_words.sort(key=lambda w: (-bm25.idf.get(w, 0.0)))\n",
    "\n",
    "for w in sample_doc_words:\n",
    "    print(w, bm25.idf.get(w, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "864b76ae-5237-42c3-99da-31882a997000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mask> <mask> <mask> -lrb- born february 12 , 1949 -rrb- is a former major league baseball player .\n",
      "he was the first-round pick of the washington <mask> in the <mask> <mask> of the june 1970 major league baseball draft , <mask> overall .\n",
      "\n",
      "\n",
      "\n",
      "<mask> <mask> <mask> -lrb- born february 12 , <mask> -rrb- is a former major league <mask> player .\n",
      "he was the first-<mask> <mask> of the washington <mask> in the <mask> <mask> of the june 1970 major league <mask> <mask> , <mask> <mask> .\n",
      "\n",
      "\n",
      "\n",
      "<mask> <mask> <mask> -<mask>- born february <mask> , <mask> -<mask>- is a former <mask> league <mask> <mask> .\n",
      "he was the first-<mask> <mask> of the <mask> <mask> in the <mask> <mask> of the june <mask> <mask> league <mask> <mask> , <mask> <mask> .\n",
      "\n",
      "\n",
      "\n",
      "<mask> <mask> <mask> -<mask>- born <mask> <mask> , <mask> -<mask>- is a <mask> <mask> <mask> <mask> <mask> .\n",
      "<mask> was the <mask>-<mask> <mask> of the <mask> <mask> in the <mask> <mask> of the <mask> <mask> <mask> <mask> <mask> <mask> , <mask> <mask> .\n",
      "\n",
      "\n",
      "\n",
      "<mask> <mask> <mask> -<mask>- <mask> <mask> <mask> , <mask> -<mask>- <mask> <mask> <mask> <mask> <mask> <mask> <mask> .\n",
      "<mask> <mask> <mask> <mask>-<mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> , <mask> <mask> .\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fixed_redact_str(text: str, words_to_mask: List[str], mask_token: str = '<mask>') -> str:\n",
    "    for w in words_to_mask:\n",
    "        text = re.sub(\n",
    "            (r'\\b{}\\b').format(re.escape(w)),\n",
    "            mask_token, text, count=0\n",
    "        )\n",
    "    return text\n",
    "\n",
    "def redact(document: str, p: float):\n",
    "    words = list(set(words_from_text(sample_doc)))\n",
    "    words.sort(key=lambda w: (-bm25.idf.get(w, 0.0)))\n",
    "    n = round(len(sample_doc_words) * p)\n",
    "    return fixed_redact_str(text=document, words_to_mask=words[:n])\n",
    "\n",
    "\n",
    "for a in [0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "    print(redact(sample_doc, a))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69404138-eebf-4916-b0ae-9479ee3331d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(bm25.idf, open('../test_100_idf.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407cc02a-10ec-4332-8513-dc296135c89c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
