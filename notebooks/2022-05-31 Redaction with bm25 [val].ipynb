{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c8f5b4f-db69-489a-becb-b0455c0b15ff",
   "metadata": {},
   "source": [
    "### BM-25 based redaction\n",
    "\n",
    "We think that the model we have now is good, but it's failing on cases where the hardest words are almost redacted in order of how difficult they are rated from BM25. We want to develop a function that looks like `redact(text: str, p: float)` where `p%` of words are redacted, **in order of importance as measured by BM-25**. This notebook is where I'll figure out how to do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c35b72f-8de5-402f-808a-ec02cc663acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a12cb1b4-021c-4666-a6a6-62f9614894b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_bio (/home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09cbe61d72d54a47bab6d1f3b222d8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/18208 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573187b9980e49beb025eea484ee161e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/18208 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca6c4cdabf84e92b429b93b2b1d6636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/18208 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c649fd3fd44d79b26083364d810753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/18207 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing corpi\n",
      "creating search index\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "import datasets\n",
    "import os\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "num_cpus = len(os.sched_getaffinity(0))\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "words_from_text_re = re.compile(r'\\b\\w+\\b')\n",
    "def words_from_text(s: str) -> List[str]:\n",
    "    assert isinstance(s, str)\n",
    "    return words_from_text_re.findall(s)\n",
    "\n",
    "def get_words_from_doc(s: str) -> List[str]:\n",
    "    words = words_from_text(s)\n",
    "    return [w for w in words]\n",
    "\n",
    "split = 'val[:100%]'\n",
    "prof_data = datasets.load_dataset('wiki_bio', split=split, version='1.2.0')\n",
    "\n",
    "def make_table_str(ex):\n",
    "    ex['table_str'] = (\n",
    "        ' '.join(ex['input_text']['table']['column_header'] + ex['input_text']['table']['content'])\n",
    "    )\n",
    "    return ex\n",
    "\n",
    "prof_data = prof_data.map(make_table_str, num_proc=num_cpus)\n",
    "profile_corpus = prof_data['table_str']\n",
    "document_corpus = prof_data['target_text']\n",
    "\n",
    "print(\"tokenizing corpi\")\n",
    "tokenized_document_corpus = [\n",
    "    get_words_from_doc(doc) for doc in document_corpus\n",
    "]\n",
    "tokenized_profile_corpus = [\n",
    "    get_words_from_doc(prof) for prof in profile_corpus\n",
    "]\n",
    "\n",
    "print(\"creating search index\")\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_profile_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf06e94e-5a3e-401d-90ee-3974e8aa8805",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointBM250kapi(BM25Okapi):\n",
    "    \"\"\"A BM250kapi that takes extra documents to calculate idf but only returns scores within initial set of documents.\n",
    "    \n",
    "    This allows us to search only among profiles but use both profiles and documents to calculate inverse document frequency\n",
    "    of terms. That's especially useful since stopwords mostly just appear in documents (and in a small set of profiles with\n",
    "    captions) but they don't provide much utility to the search.\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus, extra_corpus):\n",
    "        super().__init__(corpus + extra_corpus)\n",
    "        self.doc_freqs = self.doc_freqs[:len(corpus)] # truncate extra docs\n",
    "        self.doc_len = self.doc_len[:len(corpus)]\n",
    "        # avgdl = num_doc / self.corpus_size\n",
    "        self.avgdl = self.avgdl * (len(corpus) / (len(corpus) + len(extra_corpus)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1827240-62ae-45d5-bc49-f0b04d4a942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = JointBM250kapi(tokenized_profile_corpus, tokenized_document_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1760a7f-eb29-409f-b851-659556469ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc = document_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc71c09b-c024-4d0f-b738-5f0fa3d50c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geniza 11.483575607564859\n",
      "tulun 11.483575607564859\n",
      "khail 10.972743118543239\n",
      "907 9.537603669512086\n",
      "880 9.148104573879777\n",
      "882 8.82085046647102\n",
      "forcing 8.438847191667527\n",
      "coptic 8.071026273872993\n",
      "sell 7.785992219890188\n",
      "attached 7.753455292062404\n",
      "properties 7.551234949833269\n",
      "patriarch 6.964482690863175\n",
      "ibn 6.942878504039736\n",
      "pay 6.874054188367566\n",
      "cairo 6.731681598654302\n",
      "alexandria 6.62244420915696\n",
      "ahmad 6.388155762286122\n",
      "believed 6.3719007184695124\n",
      "building 6.001865760200759\n",
      "site 5.999081098466496\n",
      "pope 5.749224267940596\n",
      "egypt 5.747057552834824\n",
      "contributions 5.674019271720173\n",
      "heavy 5.665999665862906\n",
      "forced 5.634545049229249\n",
      "jewish 5.517714854325458\n",
      "community 5.055610386969554\n",
      "iii 4.893632987278909\n",
      "local 4.871558293919304\n",
      "see 4.8209773165335115\n",
      "become 4.766211611337895\n",
      "mark 4.538556593330421\n",
      "some 4.446395002447301\n",
      "governor 4.191793011750258\n",
      "church 4.147825009136752\n",
      "michael 3.8709312306720367\n",
      "have 3.7855983297728404\n",
      "him 3.644602442536783\n",
      "this 3.6254145725852\n",
      "st 3.2776025615567406\n",
      "later 3.2574087886629695\n",
      "time 3.1793095927982407\n",
      "lrb 2.687377947672635\n",
      "rrb 2.687377947672635\n",
      "one 2.6441844779987935\n",
      "known 2.4917276762623324\n",
      "also 2.1409738004692755\n",
      "at 1.4971555923346447\n",
      "to 1.354886311450045\n",
      "as 1.2911063308207442\n",
      "was 0.8934100058964756\n",
      "and 0.35436210079984143\n",
      "a 0.23498512481450362\n",
      "in 0.2229103104780883\n",
      "of 0.17567680506421368\n",
      "the 0.026610935701816274\n"
     ]
    }
   ],
   "source": [
    "sample_doc_words = list(set(words_from_text(sample_doc)))\n",
    "sample_doc_words.sort(key=lambda w: (-bm25.idf.get(w, 0.0)))\n",
    "\n",
    "for w in sample_doc_words:\n",
    "    print(w, bm25.idf.get(w, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "864b76ae-5237-42c3-99da-31882a997000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pope michael iii of alexandria -lrb- also known as <mask> iii -rrb- was the <mask> pope of alexandria and patriarch of the see of st. mark -lrb- <mask> -- <mask> -rrb- .\n",
      "in <mask> , the governor of egypt , ahmad ibn <mask> , forced <mask> to pay heavy contributions , <mask> him to <mask> a church and some <mask> <mask> to the local jewish community .\n",
      "this building was at one time believed to have later become the site of the cairo <mask> .\n",
      "\n",
      "\n",
      "\n",
      "<mask> michael iii of <mask> -lrb- also known as <mask> iii -rrb- was the <mask> <mask> of <mask> and <mask> of the see of st. mark -lrb- <mask> -- <mask> -rrb- .\n",
      "in <mask> , the governor of <mask> , <mask> <mask> <mask> , forced <mask> to <mask> heavy contributions , <mask> him to <mask> a church and some <mask> <mask> to the local jewish community .\n",
      "this <mask> was at one time <mask> to have later become the <mask> of the <mask> <mask> .\n",
      "\n",
      "\n",
      "\n",
      "<mask> michael <mask> of <mask> -lrb- also known as <mask> <mask> -rrb- was the <mask> <mask> of <mask> and <mask> of the <mask> of st. <mask> -lrb- <mask> -- <mask> -rrb- .\n",
      "in <mask> , the <mask> of <mask> , <mask> <mask> <mask> , <mask> <mask> to <mask> <mask> <mask> , <mask> him to <mask> a church and <mask> <mask> <mask> to the <mask> <mask> <mask> .\n",
      "this <mask> was at one time <mask> to have later <mask> the <mask> of the <mask> <mask> .\n",
      "\n",
      "\n",
      "\n",
      "<mask> <mask> <mask> of <mask> -<mask>- also known as <mask> <mask> -<mask>- was the <mask> <mask> of <mask> and <mask> of the <mask> of <mask>. <mask> -<mask>- <mask> -- <mask> -<mask>- .\n",
      "in <mask> , the <mask> of <mask> , <mask> <mask> <mask> , <mask> <mask> to <mask> <mask> <mask> , <mask> <mask> to <mask> a <mask> and <mask> <mask> <mask> to the <mask> <mask> <mask> .\n",
      "<mask> <mask> was at <mask> <mask> <mask> to <mask> <mask> <mask> the <mask> of the <mask> <mask> .\n",
      "\n",
      "\n",
      "\n",
      "<mask> <mask> <mask> <mask> <mask> -<mask>- <mask> <mask> <mask> <mask> <mask> -<mask>- <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask>. <mask> -<mask>- <mask> -- <mask> -<mask>- .\n",
      "<mask> <mask> , <mask> <mask> <mask> <mask> , <mask> <mask> <mask> , <mask> <mask> <mask> <mask> <mask> <mask> , <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> .\n",
      "<mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> <mask> .\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fixed_redact_str(text: str, words_to_mask: List[str], mask_token: str = '<mask>') -> str:\n",
    "    for w in words_to_mask:\n",
    "        text = re.sub(\n",
    "            (r'\\b{}\\b').format(re.escape(w)),\n",
    "            mask_token, text, count=0\n",
    "        )\n",
    "    return text\n",
    "\n",
    "def redact(document: str, p: float):\n",
    "    words = list(set(words_from_text(sample_doc)))\n",
    "    words.sort(key=lambda w: (-bm25.idf.get(w, 0.0)))\n",
    "    n = round(len(sample_doc_words) * p)\n",
    "    return fixed_redact_str(text=document, words_to_mask=words[:n])\n",
    "\n",
    "\n",
    "for a in [0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "    print(redact(sample_doc, a))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69404138-eebf-4916-b0ae-9479ee3331d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(bm25.idf, open('../val_100_idf.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407cc02a-10ec-4332-8513-dc296135c89c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
