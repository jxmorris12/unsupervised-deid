{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9b215f-3c27-4098-b6cd-507e106f93d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jxm3/research/deidentification/unsupervised-deidentification')\n",
    "\n",
    "from dataloader import WikipediaDataModule\n",
    "from model import AbstractModel, CoordinateAscentModel\n",
    "from utils import get_profile_embeddings_by_model_key\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from model_cfg import model_paths_dict\n",
    "\n",
    "datasets.utils.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "num_cpus = len(os.sched_getaffinity(0))\n",
    "\n",
    "\n",
    "def get_profile_embeddings(model_key: str):\n",
    "    profile_embeddings = get_profile_embeddings_by_model_key(model_key=model_key)\n",
    "\n",
    "    print(\"concatenating train, val, and test profile embeddings\")\n",
    "    all_profile_embeddings = torch.cat(\n",
    "        (profile_embeddings['test'], profile_embeddings['val'], profile_embeddings['train']), dim=0\n",
    "    )\n",
    "\n",
    "    print(\"all_profile_embeddings:\", all_profile_embeddings.shape)\n",
    "    return all_profile_embeddings\n",
    "\n",
    "def load_adv_csv(dm: WikipediaDataModule, max_num_samples: int = 100) -> pd.DataFrame:\n",
    "    # Load adv CSVs\n",
    "    adv_df = None\n",
    "    adv_csvs_folder = os.path.normpath(\n",
    "        os.path.join(\n",
    "            os.getcwd(), os.pardir, 'adv_csvs_full_8'\n",
    "        )\n",
    "    )\n",
    "    print('adv_csvs_folder', adv_csvs_folder)\n",
    "    model_csv_filenames = list(glob.glob(\n",
    "        os.path.join(\n",
    "            adv_csvs_folder,\n",
    "            'model*/results*.csv'\n",
    "        )\n",
    "    ) )\n",
    "    print(\"len(model_csv_filenames) =\", len(model_csv_filenames))\n",
    "    for filename in (model_csv_filenames):\n",
    "        print('filename:', filename)\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "        if len(df) < 50: continue\n",
    "        \n",
    "        df = df[(df['result_type'] == 'Successful') | (df['result_type'] == 'Skipped')]\n",
    "        json_filename = filename.replace('.csv', '__args.json')\n",
    "        assert os.path.exists(json_filename)\n",
    "        \n",
    "        adv_csv_json = json.load(open(json_filename, 'r'))\n",
    "        \n",
    "        df[\"filename\"] = filename\n",
    "        for key, val in adv_csv_json.items():\n",
    "            df[key] = val\n",
    "        df[\"i\"] = df.index\n",
    "        \n",
    "        df = df.rename(columns={\"model\": \"model_name\"})\n",
    "\n",
    "        mini_df = df.iloc[:max_num_samples]\n",
    "\n",
    "        if adv_df is None:\n",
    "            adv_df = mini_df\n",
    "        else:\n",
    "            adv_df = pd.concat((adv_df, mini_df), axis=0)\n",
    "        # print(adv_df)\n",
    "    \n",
    "    # Load baseline redacted data\n",
    "    mini_test_dataset = dm.test_dataset[:max_num_samples]\n",
    "    doc_df = pd.DataFrame(\n",
    "        columns=['perturbed_text'],\n",
    "        data=mini_test_dataset['document']\n",
    "    )\n",
    "    doc_df['model_name'] = 'document'\n",
    "    doc_df['i'] = doc_df.index\n",
    "    doc_df['filename'] = 'document'\n",
    "    \n",
    "    ner_df = pd.DataFrame(\n",
    "        columns=['perturbed_text'],\n",
    "        data=mini_test_dataset['document_redact_ner_bert']\n",
    "    )\n",
    "    ner_df['model_name'] = 'named_entity'\n",
    "    ner_df['i'] = ner_df.index\n",
    "    ner_df['filename'] = 'named_entity'\n",
    "        \n",
    "    lex_df = pd.DataFrame(\n",
    "        columns=['perturbed_text'],\n",
    "        data=mini_test_dataset['document_redact_lexical']\n",
    "    )\n",
    "    lex_df['model_name'] = 'lexical'\n",
    "    lex_df['i'] = lex_df.index\n",
    "    lex_df = lex_df.iloc[:max_num_samples]\n",
    "    lex_df['filename'] = 'lexical'\n",
    "\n",
    "    # Combine both adversarial and baseline redacted data\n",
    "    baseline_df = pd.concat((doc_df, lex_df, ner_df), axis=0)\n",
    "    for key in adv_csv_json.keys():\n",
    "        if key not in baseline_df: baseline_df[key] = None\n",
    "        \n",
    "    \n",
    "    full_df = pd.concat((adv_df, baseline_df), axis=0)\n",
    "    \n",
    "    # Put newlines back\n",
    "    full_df['perturbed_text'] = full_df['perturbed_text'].apply(lambda s: s.replace('<SPLIT>', '\\n'))\n",
    "\n",
    "    # Standardize mask tokens\n",
    "    full_df['perturbed_text'] = full_df['perturbed_text'].apply(lambda s: s.replace('[MASK]', dm.mask_token))\n",
    "    full_df['perturbed_text'] = full_df['perturbed_text'].apply(lambda s: s.replace('<mask>', dm.mask_token))\n",
    "    \n",
    "    # Fair truncation\n",
    "    full_df['original_num_words'] = full_df['perturbed_text'].map(lambda s: len(s.split()))\n",
    "    for i in full_df['i'].unique():\n",
    "        #         df.loc[df.loc[df['a'] == 1,'b'].index[1], 'b'] = 3\n",
    "        min_num_words = full_df[full_df['i'] == i]['original_num_words'].min()\n",
    "        full_df.loc[full_df[full_df['i'] == i].index, 'perturbed_text'] = (\n",
    "            full_df.loc[full_df[full_df['i'] == i].index, 'perturbed_text'].map(\n",
    "                lambda t: ' '.join(t.split()[:min_num_words])\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    full_df['num_words'] = full_df['perturbed_text'].map(lambda s: len(s.split()))\n",
    "    \n",
    "    \n",
    "    # This makes sure sure all documents with a given index have the same number of words.\n",
    "    assert full_df.groupby('i')['num_words'].std().max() == 0.0\n",
    "\n",
    "    return full_df\n",
    "\n",
    "\n",
    "def get_adv_predictions(model_key: str, max_num_samples: int):\n",
    "    checkpoint_path = model_paths_dict[model_key]\n",
    "    assert isinstance(checkpoint_path, str), f\"invalid checkpoint_path {checkpoint_path} for {model_key}\"\n",
    "    print(f\"running eval on {model_key} loaded from {checkpoint_path}\")\n",
    "    model = CoordinateAscentModel.load_from_checkpoint(\n",
    "        checkpoint_path\n",
    "    )\n",
    "\n",
    "    print(f\"loading data with {num_cpus} CPUs\")\n",
    "    dm = WikipediaDataModule(\n",
    "        document_model_name_or_path=model.document_model_name_or_path,\n",
    "        profile_model_name_or_path=model.profile_model_name_or_path,\n",
    "        dataset_name='wiki_bio',\n",
    "        dataset_train_split='train[:256]',\n",
    "        dataset_val_split='val[:256]',\n",
    "        dataset_test_split='test[:100%]',\n",
    "        dataset_version='1.2.0',\n",
    "        num_workers=num_cpus,\n",
    "        train_batch_size=256,\n",
    "        eval_batch_size=256,\n",
    "        max_seq_length=128,\n",
    "        sample_spans=False,\n",
    "    )\n",
    "    dm.setup(\"fit\")\n",
    "\n",
    "    adv_csv = load_adv_csv(dm=dm, max_num_samples=max_num_samples)\n",
    "\n",
    "    all_profile_embeddings = get_profile_embeddings(model_key=model_key).cuda()\n",
    "\n",
    "    model.document_model.eval()\n",
    "    model.document_model.cuda()\n",
    "    model.document_embed.eval()\n",
    "    model.document_embed.cuda()\n",
    "\n",
    "    topk_values = []\n",
    "    topk_idxs = []\n",
    "    batch_size = 256\n",
    "    i = 0\n",
    "    while i < len(adv_csv):\n",
    "        ex = adv_csv.iloc[i:i+batch_size]\n",
    "        test_batch = dm.document_tokenizer.batch_encode_plus(\n",
    "            ex['perturbed_text'].tolist(),\n",
    "            max_length=dm.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        test_batch = {\n",
    "            f'perturbed_text__{k}': v for k,v in test_batch.items()\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            document_embeddings = model.forward_document(batch=test_batch, document_type='perturbed_text')\n",
    "            document_to_profile_logits = document_embeddings @ all_profile_embeddings.T\n",
    "            document_to_profile_probs = document_to_profile_logits.softmax(dim=1)\n",
    "            topk_10 = document_to_profile_probs.topk(10)\n",
    "            topk_values.append(topk_10.values)\n",
    "            topk_idxs.append(topk_10.indices)\n",
    "\n",
    "        i += batch_size\n",
    "    \n",
    "    adv_csv['pred_topk_values'] = torch.cat(topk_values, dim=0).cpu().tolist()\n",
    "    adv_csv['pred_topk_idxs'] = torch.cat(topk_idxs, dim=0).cpu().tolist()\n",
    "    return adv_csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3085ba46-cac1-40e0-a419-9099fe2395f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running eval on model_3_2 loaded from /home/jxm3/research/deidentification/unsupervised-deidentification/saves/ca__roberta__tapas__dropout_-1.0_1.0_0.0__e3072__ls0.1/deid-wikibio-4_lightning_logs/ojgxa1tf_6/checkpoints/epoch=65-step=150282-idf_total.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model with learning_rate = 0.0001 and patience 6\n",
      "loading data with 8 CPUs\n",
      "Initializing WikipediaDataModule with num_workers = 8 and mask token `<mask>`\n",
      "loading wiki_bio[1.2.0] split train[:256]\n",
      "loading wiki_bio[1.2.0] split val[:256]\n",
      "loading wiki_bio[1.2.0] split test[:100%]\n",
      "                        adv_csvs_folder /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8\n",
      "len(model_csv_filenames) = 35\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_None__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__k_0__n_1000__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_1__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_0__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_2__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_3__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_4__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_8__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_13__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_16__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts1.0__idf0.5__k_1__n_10__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts1.0__idf0.5__k_None__n_10__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts1.0__k_None__n_10__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_24__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_32__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__k_0__n_100__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__mig12.0__epsNone__k_0__n_1000__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__eps0.7__k_None__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__mig6.0__epsNone__k_0__n_1000__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__eps0.02__k_None__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__mig11.0__epsNone__k_0__n_1000__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__eps0.5__k_None__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__eps0.01__k_None__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__eps0.2__k_None__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__mig10.0__epsNone__k_0__n_1000__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__mig5.0__epsNone__k_0__n_1000__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__eps0.1__k_None__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__eps0.005__k_None__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__mig9.0__epsNone__k_0__n_1000__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__eps0.05__k_None__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__eps0.001__k_None__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__mig8.0__epsNone__k_0__n_1000__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__mig4.0__epsNone__k_0__n_1000__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__mig7.0__epsNone__k_0__n_1000__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__mig3.5__epsNone__k_0__n_1000__type_swap.csv\n",
      ">> loaded 582659 train embeddings from /home/jxm3/research/deidentification/unsupervised-deidentification/embeddings/profile/model_3_2/train.pkl\n",
      ">> loaded 72831 val embeddings from /home/jxm3/research/deidentification/unsupervised-deidentification/embeddings/profile/model_3_2/val.pkl\n",
      ">> loaded 72831 test embeddings from /home/jxm3/research/deidentification/unsupervised-deidentification/embeddings/profile/model_3_2/test.pkl\n",
      "concatenating train, val, and test profile embeddings\n",
      "all_profile_embeddings: torch.Size([728321, 3072])\n",
      "running eval on model_3_3 loaded from /home/jxm3/research/deidentification/unsupervised-deidentification/saves/ca__roberta__dropout_-1.0_1.0_0.0__e3072__ls0.1/deid-wikibio-4_lightning_logs/2cr1gp87_28/checkpoints/epoch=68-step=157113.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model with learning_rate = 0.0001 and patience 6\n",
      "loading data with 8 CPUs\n",
      "Initializing WikipediaDataModule with num_workers = 8 and mask token `<mask>`\n",
      "loading wiki_bio[1.2.0] split train[:256]\n",
      "loading wiki_bio[1.2.0] split val[:256]\n",
      "loading wiki_bio[1.2.0] split test[:100%]\n",
      "                        adv_csvs_folder /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8\n",
      "len(model_csv_filenames) = 35\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_None__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__k_0__n_1000__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_1__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_0__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_2__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_3__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_4__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_8__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_13__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_16__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts1.0__idf0.5__k_1__n_10__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts1.0__idf0.5__k_None__n_10__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts1.0__k_None__n_10__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_24__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__k_32__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__k_0__n_100__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__mig12.0__epsNone__k_0__n_1000__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__eps0.7__k_None__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__mig6.0__epsNone__k_0__n_1000__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__eps0.02__k_None__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__mig11.0__epsNone__k_0__n_1000__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__eps0.5__k_None__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__eps0.01__k_None__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__eps0.2__k_None__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__mig10.0__epsNone__k_0__n_1000__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__mig5.0__epsNone__k_0__n_1000__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__eps0.1__k_None__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__eps0.005__k_None__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__mig9.0__epsNone__k_0__n_1000__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__eps0.05__k_None__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_2__ts10.0__idf0.0__mp0.95__eps0.001__k_None__n_200__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__mig8.0__epsNone__k_0__n_1000__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__mig4.0__epsNone__k_0__n_1000__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__mig7.0__epsNone__k_0__n_1000__type_swap.csv\n",
      "filename: /home/jxm3/research/deidentification/unsupervised-deidentification/adv_csvs_full_8/model_3_4/results__b_1__ts100.0__nomodel__idf0.0__mp1.0__mig3.5__epsNone__k_0__n_1000__type_swap.csv\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "roberta_tapas_predictions = get_adv_predictions(model_key='model_3_2', max_num_samples=n)\n",
    "roberta_roberta_predictions = get_adv_predictions(model_key='model_3_3', max_num_samples=n)\n",
    "pmlm_tapas_predictions = get_adv_predictions(model_key='model_3_4', max_num_samples=n)\n",
    "# TODO: drop predictions if we predicted the same thing with same hparams just different values of n. Like\n",
    "# if we have filenames with n=1000 and n=100. Just take the one with largest n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790db219-6d01-4ef2-ae8d-b14179284c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_name = {\n",
    "    'model_3_1': 'roberta_tapas__no_masking',\n",
    "    'model_3_2': 'roberta_tapas',\n",
    "    'model_3_3': 'roberta_roberta',\n",
    "    'model_3_4': 'pmlm_tapas',\n",
    "}\n",
    "\n",
    "roberta_roberta_predictions['model_name'] = roberta_roberta_predictions['model_name'].apply(lambda s: new_model_name.get(s, s))\n",
    "\n",
    "out_df = roberta_roberta_predictions.rename(\n",
    "    columns={'pred_topk_values': 'roberta_roberta__pred_topk_values', 'pred_topk_idxs': 'roberta_roberta__pred_topk_idxs'}\n",
    ")\n",
    "\n",
    "\n",
    "out_df['pmlm_tapas__pred_topk_values'] = pmlm_tapas_predictions['pred_topk_values']\n",
    "out_df['pmlm_tapas__pred_topk_idxs'] = pmlm_tapas_predictions['pred_topk_idxs']\n",
    "\n",
    "out_df['roberta_roberta__pred_topk_values'] = roberta_roberta_predictions['pred_topk_values']\n",
    "out_df['roberta_roberta__pred_topk_idxs'] = roberta_roberta_predictions['pred_topk_idxs']\n",
    "\n",
    "out_df['roberta_tapas__pred_topk_values'] = roberta_tapas_predictions['pred_topk_values']\n",
    "out_df['roberta_tapas__pred_topk_idxs'] = roberta_tapas_predictions['pred_topk_idxs']\n",
    "\n",
    "out_df['pmlm_tapas__was_correct'] = out_df.apply(lambda row: row['i'] == row['pmlm_tapas__pred_topk_idxs'][0], axis=1)\n",
    "out_df['roberta_roberta__was_correct'] = out_df.apply(lambda row: row['i'] == row['roberta_roberta__pred_topk_idxs'][0], axis=1)\n",
    "out_df['roberta_tapas__was_correct'] = out_df.apply(lambda row: row['i'] == row['roberta_tapas__pred_topk_idxs'][0], axis=1)\n",
    "\n",
    "\n",
    "# out_df = out_df.drop(columns=['level_0', 'index', 'Unnamed: 0'])\n",
    "out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f4d26-91d9-40f3-a6b3-f0516e9cf80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_df['all_models_were_correct'] = out_df['roberta_roberta__was_correct'] & out_df['roberta_tapas__was_correct'] & out_df['pmlm_tapas__was_correct']\n",
    "out_df['any_model_was_correct'] = out_df['roberta_roberta__was_correct'] | out_df['roberta_tapas__was_correct'] | out_df['pmlm_tapas__was_correct']\n",
    "\n",
    "out_df['num_masks'] = out_df['perturbed_text'].map(lambda s: s.count('<mask>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef85c047-7781-4af2-9a45-1e925b101489",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df['model_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d1ca5-01bc-42f2-bc52-b3579d10925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "sns.set_theme(context=\"paper\", style=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd8d018-b08e-4d42-a911-b5bc2c60d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = out_df.groupby(\"filename\").mean().reset_index()\n",
    "\n",
    "df_grouped[\"percent_masks\"] = df_grouped[\"num_masks\"] / df_grouped[\"num_words\"]\n",
    "df_grouped[\"percent_not_masks\"] = 1.0 - df_grouped[\"percent_masks\"]\n",
    "\n",
    "\n",
    "\n",
    "def name_by_filename(filename: str) -> str:\n",
    "    if filename in ['document', 'lexical', 'named_entity']:\n",
    "        return filename\n",
    "    elif 'nomodel' in filename:\n",
    "        return 'IDF'\n",
    "    elif 'k_None' in filename:\n",
    "        return 'eps'\n",
    "    elif '5__k' in filename:\n",
    "        return 'k'\n",
    "    else:\n",
    "        return '??'\n",
    "\n",
    "df_grouped[\"filename_short\"] = df_grouped.apply(lambda row: name_by_filename(row['filename']), axis=1)\n",
    "# df_grouped[\"filename_short\"] = df_grouped[\"filename\"].map(lambda s: s[s.rindex('/')+1:s.rindex('.csv')] if '.csv' in s else s) # TODO renamee\n",
    "# df_grouped[\"model_name__k\"] = df_grouped[\"model_name\"] + df_grouped[\"k\"].map(lambda k: f'__{k}')\n",
    "\n",
    "# https://matplotlib.org/stable/api/markers_api.html\n",
    "marker_by_model_name = {\n",
    "    \"lexical\": \"s\", #\"X\",\n",
    "    \"named_entity\": \"s\", # \"X\",\n",
    "    \"document\": \"s\"\n",
    "}\n",
    "df_grouped[\"marker\"] = df_grouped.apply(lambda row: marker_by_model_name.get(row[\"filename_short\"], \"o\"), axis=1)\n",
    "\n",
    "num_filenames = len(df_grouped[\"filename_short\"].unique())\n",
    "color_by_model_name = dict(\n",
    "    zip(df_grouped[\"filename_short\"].unique(), sns.color_palette(\"hls\", num_filenames))\n",
    ")\n",
    "df_grouped[\"color\"] = df_grouped[\"filename_short\"].apply(color_by_model_name.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4b4849-0ba8-418a-9b59-49a60128c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_reid_plot(reid_model_name: str, x_column: str, xlabel: str, ylabel: str, marker_size: int = 150, set_lim=True):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.xlabel(xlabel, fontsize=14)\n",
    "    plt.ylabel(ylabel, fontsize=14)\n",
    "    \n",
    "    group_key = \"filename_short\"\n",
    "    \n",
    "    # df_grouped_filtered = df_grouped\n",
    "    df_grouped_filtered = df_grouped[\n",
    "        df_grouped[group_key].apply(lambda row_model_name: (reid_model_name not in row_model_name))\n",
    "    ]\n",
    "    # print(df_grouped_filtered)\n",
    "    y_column = f\"{reid_model_name}__was_correct\"\n",
    "    \n",
    "    g1 = sns.lineplot(\n",
    "        data=df_grouped_filtered,\n",
    "        x=x_column,\n",
    "        y=y_column,\n",
    "        hue=group_key,\n",
    "        palette=color_by_model_name,\n",
    "        linewidth=5,\n",
    "        legend=True\n",
    "    )\n",
    "    # g1.set_xscale('log')\n",
    "    g1.legend(loc='upper right')\n",
    "    if set_lim: g1.set(xlim=(.18,.8), ylim=(-0.05, 0.45))\n",
    "    for marker_type in df_grouped[\"marker\"].unique():\n",
    "        df_grouped_marker = df_grouped_filtered[\n",
    "            df_grouped_filtered[\"marker\"] == marker_type\n",
    "        ]\n",
    "        g2 = sns.scatterplot(\n",
    "            data=df_grouped_marker,\n",
    "            x=x_column,\n",
    "            y=y_column,\n",
    "            hue=group_key,\n",
    "            palette=color_by_model_name,\n",
    "            s=(marker_size if marker_type == 'o' else marker_size*1.5),\n",
    "            marker=marker_type,\n",
    "            legend=False\n",
    "        )\n",
    "        # g2.set_xscale('log')\n",
    "    if set_lim: g2.set(xlim=(.18,.8), ylim=(-0.05, 0.45))\n",
    "\n",
    "\n",
    "make_reid_plot(reid_model_name=\"roberta_roberta\", x_column=\"percent_masks\", ylabel=\"Reidentification % (RoBERTa-RoBERTa)\", xlabel=\"% Words masked\", set_lim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3022d2e-9386-4c92-a717-5f48d057fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_reid_plot(reid_model_name=\"any_model\", x_column=\"percent_masks\", ylabel=\"Reidentification % (Any Model)\", xlabel=\"% Words masked\", set_lim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e957e18-424f-420f-83d6-fe62033faa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_reid_plot(reid_model_name=\"roberta_roberta\", x_column=\"num_masks\", ylabel=\"Reidentification % (RoBERTa-RoBERTa)\", xlabel=\"Num words masked\", set_lim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9914a22f-08fc-4db2-aeb1-9b5962086d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "((out_df[\"model_name\"] == \"pmlm_tapas\") & (out_df[\"k\"] == 1) & (out_df[\"p\"] == 0.2)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba864131-9be3-4edd-a439-b7b1c2c2b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece14fd-7dcb-4cd5-bffe-98447a961b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped.sort_values(by='roberta_roberta__was_correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a148d018-16d1-44f2-98f6-84b3433b2729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
