{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44c62c0e-7de7-4152-820a-7c07b192987e",
   "metadata": {},
   "source": [
    "## More tests trying to figure out how my model is so good at redacted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88be6903-9751-4601-8f3c-f95dd21452d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing WikipediaDataModule with num_workers = 4 and mask token `<mask>`\n",
      "loading wiki_bio[1.2.0] split train[:100%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_bio (/home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading wiki_bio[1.2.0] split val[:20%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_bio (/home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da)\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-3c0ffadd02c12daf.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-7d07543b6205ca87.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-7440752484ad8676.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-2c6f94b0d2dcc153.arrow\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jxm3/research/deidentification/unsupervised-deidentification')\n",
    "\n",
    "from datamodule import WikipediaDataModule\n",
    "\n",
    "import os\n",
    "\n",
    "num_cpus = os.cpu_count()\n",
    "dm = WikipediaDataModule(\n",
    "    document_model_name_or_path=\"roberta-base\",\n",
    "    profile_model_name_or_path=\"google/tapas-base\",\n",
    "    max_seq_length=128,\n",
    "    dataset_name='wiki_bio',\n",
    "    dataset_train_split='train[:100%]',\n",
    "    dataset_val_split='val[:20%]',\n",
    "    dataset_version='1.2.0',\n",
    "    word_dropout_ratio=0.0,\n",
    "    word_dropout_perc=0.0,\n",
    "    num_workers=4,\n",
    "    train_batch_size=512,\n",
    "    eval_batch_size=512\n",
    ")\n",
    "dm.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea37bab-f3c2-4cf7-adf2-a432fe15c040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model with learning_rate = 1e-05 and patience 1\n"
     ]
    }
   ],
   "source": [
    "from model import CoordinateAscentModel\n",
    "\n",
    "# model that was trained at the link given above, gets >99% validation accuracy,\n",
    "# and is trained with word dropout!\n",
    "\n",
    "from model_cfg import model_paths_dict\n",
    "\n",
    "checkpoint_path = model_paths_dict[\"model_5\"]\n",
    "\n",
    "model = CoordinateAscentModel.load_from_checkpoint(\n",
    "    checkpoint_path,\n",
    "    document_model_name_or_path=\"roberta-base\",\n",
    "    profile_model_name_or_path=\"google/tapas-base\",\n",
    "    learning_rate=1e-5,\n",
    "    pretrained_profile_encoder=False,\n",
    "    lr_scheduler_factor=0.5,\n",
    "    lr_scheduler_patience=1,\n",
    "    train_batch_size=1,\n",
    "    num_workers=8,\n",
    "    gradient_clip_val=10.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa42a93e-e5d8-4d7c-9dc0-c026b54cc0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jxm3/.conda/envs/torch/lib/python3.9/site-packages/tqdm/std.py:163: TqdmWarning: Unknown colour (orange); valid choices: [hex (#00ff00), BLACK, RED, GREEN, YELLOW, BLUE, MAGENTA, CYAN, WHITE]\n",
      "  self.colour = colour\n",
      "                                                                            1.42s/it]\r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "def precompute_profile_embeddings():\n",
    "    model.profile_model.cuda()\n",
    "    model.profile_model.eval()\n",
    "\n",
    "    model.train_profile_embeddings = np.zeros((len(dm.train_dataset), model.profile_embedding_dim))'\n",
    "    for train_batch in tqdm.tqdm(dm.train_dataloader(), desc=\"Precomputing train embeddings\", colour=\"yellow\", leave=False):\n",
    "        with torch.no_grad():\n",
    "            profile_embeddings = model.forward_profile(batch=train_batch)\n",
    "        model.train_profile_embeddings[train_batch[\"text_key_id\"]] = profile_embeddings.cpu()\n",
    "    model.train_profile_embeddings = torch.tensor(model.train_profile_embeddings, dtype=torch.float32)\n",
    "\n",
    "    model.val_profile_embeddings = np.zeros((len(dm.val_dataset), model.profile_embedding_dim))\n",
    "    for val_batch in tqdm.tqdm(dm.val_dataloader()[0], desc=\"Precomputing val embeddings\", colour=\"green\", leave=False):\n",
    "        with torch.no_grad():\n",
    "            profile_embeddings = model.forward_profile(batch=val_batch)\n",
    "        model.val_profile_embeddings[val_batch[\"text_key_id\"]] = profile_embeddings.cpu()\n",
    "    model.val_profile_embeddings = torch.tensor(model.val_profile_embeddings, dtype=torch.float32)\n",
    "    model.profile_model.train()\n",
    "\n",
    "precompute_profile_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cbb38f9-52b0-4090-84f0-056264a175b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([582659, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_profile_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9f48ec7-1fd3-4141-9b23-ae3c8b8a5dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14566, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.val_profile_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c62b5fd9-0366-44d9-8966-b189dea6cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"<mask> a. <mask> (<mask> <mask> , <mask> -- <mask> <mask> , <mask>) was a <mask> <mask> <mask> <mask> and a <mask> .\\nmost <mask> , he <mask> from <mask> to <mask> as <mask> <mask> of <mask> <mask> , from <mask> to <mask> as the <mask> <mask> <mask> to the <mask> <mask> and from <mask> to <mask> as <mask> to the <mask> <mask> of <mask> during the <mask> <mask> of <mask> <mask> .\\n<mask> <mask> to the <mask> of <mask> <mask> in the <mask>.s. <mask> and is a <mask> of the <mask> <mask> <mask> of <mask> .\"\n",
    "doc_tokenized = dm.document_tokenizer(doc, max_length=dm.max_seq_length, truncation=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    doc_emb = model.forward_document_inputs(inputs=doc_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6801c59-4823-49da-8b65-0cfdbe46582d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3588e-16, 1.6434e-11, 5.9551e-10,  ..., 2.1308e-11, 3.3903e-12,\n",
       "         3.8873e-10]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_sims = (doc_emb @ model.val_profile_embeddings.T)\n",
    "val_sims.softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88e24b3e-1005-42f5-b9f4-f642d2c297dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9280), tensor(98))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_sims.softmax(dim=1).max(), val_sims.softmax(dim=1).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e4dced6-b25a-426e-b6fd-9f6bb1b83652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vernon A. Walters'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.val_dataset[98]['name'] # correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31121d6d-d16f-4b78-b01f-400d12c8d7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4031), tensor(455976))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sims = (doc_emb @ model.train_profile_embeddings.T)\n",
    "train_sims.softmax(dim=1).max(), train_sims.softmax(dim=1).argmax(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c426cebc-b1ab-4d88-9277-2c913812163f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Michael A. Sheehan'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.train_dataset[455976]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3aad4cf-abf0-4132-9bec-65d94f34cf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'michael a. sheehan ( born february 10 , 1955 ) is a united states author and former government official and military officer .\\nhe is currently distinguished chair at the u.s. military academy in west point and a terrorist analyst for nbc news .\\nhe was born in red bank , new jersey .\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.train_dataset[455976]['document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1005baa-37b0-4c92-a7e2-54bd841ea6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vernon a. walters ( january 3 , 1917 -- february 10 , 2002 ) was a united states army officer and a diplomat .\\nmost notably , he served from 1972 to 1976 as deputy director of central intelligence , from 1985 to 1989 as the united states ambassador to the united nations and from 1989 to 1991 as ambassador to the federal republic of germany during the decisive phase of german reunification .\\nwalters rose to the rank of lieutenant general in the u.s. army and is a member of the military intelligence hall of fame .\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.val_dataset[98]['document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef686193-da34-4244-aa2e-39203fa6c3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<mask> a. <mask> (<mask> <mask> , <mask> -- <mask> <mask> , <mask>) was a <mask> <mask> <mask> <mask> and a <mask> .\\nmost <mask> , he <mask> from <mask> to <mask> as <mask> <mask> of <mask> <mask> , from <mask> to <mask> as the <mask> <mask> <mask> to the <mask> <mask> and from <mask> to <mask> as <mask> to the <mask> <mask> of <mask> during the <mask> <mask> of <mask> <mask> .\\n<mask> <mask> to the <mask> of <mask> <mask> in the <mask>.s. <mask> and is a <mask> of the <mask> <mask> <mask> of <mask> .'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7d77463-e37c-463d-b496-d03bc6ebc230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4031, 0.1569, 0.1311, 0.0178, 0.0162, 0.0152, 0.0140, 0.0137, 0.0105,\n",
       "         0.0100]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sims.softmax(dim=1).sort().values.flip(-1)[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca75d94-340b-49dc-b755-f52b2cec8bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
