{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54dd8d98-0748-478b-8c82-6b77cfa44e53",
   "metadata": {},
   "source": [
    "### Constrative cross-encoder redaction test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d252cb9-4ee5-42f2-98df-af6e72555090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jxm3/research/deidentification/unsupervised-deidentification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8309e7b9-066d-4056-8522-4e64934bb158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: set num_workers to 1, expect dataloader bottleneck\n",
      "Initializing WikipediaDataModule with num_workers = 1 and mask token `<mask>`\n",
      "loading wiki_bio[1.2.0] split train[:256]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_bio (/home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading wiki_bio[1.2.0] split val[:20%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_bio (/home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da)\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-wiki_bio_train__256__1.2.0__wiki.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-wiki_bio_val__20___1.2.0__wiki.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-wiki_bio_val__20___1.2.0__lexical_redacted.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-wiki_bio_val__20___1.2.0__spacy_redacted.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-wiki_bio_val__20___1.2.0__idf20_redacted.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-wiki_bio_val__20___1.2.0__idf40_redacted.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-wiki_bio_val__20___1.2.0__idf60_redacted.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-wiki_bio_val__20___1.2.0__idf80_redacted.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-wiki_bio_train__256__1.2.0__128_roberta-base_roberta-base_tokenized.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-wiki_bio_val__20___1.2.0__128_roberta-base_roberta-base_tokenized.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train nearest-neighbors from: /home/jxm3/research/deidentification/unsupervised-deidentification/nearest_neighbors/nn__train[:256]__256.p\n",
      "Loading val nearest-neighbors from: /home/jxm3/research/deidentification/unsupervised-deidentification/nearest_neighbors/nn__val[:20%]__256.p\n"
     ]
    }
   ],
   "source": [
    "from dataloader import WikipediaDataModule\n",
    "import os\n",
    "\n",
    "num_cpus = len(os.sched_getaffinity(0))\n",
    "\n",
    "dm = WikipediaDataModule(\n",
    "    document_model_name_or_path=\"roberta-base\",\n",
    "    profile_model_name_or_path=\"roberta-base\",\n",
    "    max_seq_length=128,\n",
    "    dataset_name='wiki_bio',\n",
    "    dataset_train_split='train[:256]', # not used in this notebook\n",
    "    dataset_val_split='val[:20%]',\n",
    "    dataset_version='1.2.0',\n",
    "    word_dropout_ratio=0.0,\n",
    "    word_dropout_perc=0.0,\n",
    "    num_workers=1,\n",
    "    num_nearest_neighbors=16,\n",
    "    train_batch_size=64,\n",
    "    eval_batch_size=64\n",
    ")\n",
    "dm.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a13cb91-0d89-4fc5-b95e-ba496ccf3ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../saves/cca__roberta__idf__n_16__dropout_-1.0_1.0_0.0/deid-wikibio-4-cross-encoder_default/2jda7fn3_777/checkpoints/last.ckpt',\n",
       " '../saves/cca__roberta__idf__n_16__dropout_-1.0_1.0_0.0/deid-wikibio-4-cross-encoder_default/2jda7fn3_777/checkpoints/epoch=5-step=43703-idf_total.ckpt',\n",
       " '../saves/cca__roberta__idf__n_16__dropout_-1.0_1.0_0.0/deid-wikibio-4-cross-encoder_default/2jda7fn3_777/checkpoints/epoch=8-step=65555-idf_total.ckpt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "# glob.glob(\"../saves/cca__roberta__idf__n_8__dropout_-1.0_1.0_0.0/*/*_775/checkpoints/*.ckpt\")\n",
    "glob.glob(\"../saves/cca__roberta__idf__n_16__dropout_-1.0_1.0_0.0/*/*_777/checkpoints/*.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a87966-0c59-45f7-8502-5e7da6710a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing contrastive cross-encoder as document model (not creating profile model)\n",
      "Initialized ContrastiveCrossAttention model with learning_rate = 2e-05 and patience 6\n"
     ]
    }
   ],
   "source": [
    "from model import ContrastiveCrossAttentionModel\n",
    "\n",
    "import glob\n",
    "\n",
    "# this model only trained with 1% of data -- here, after 19 epochs\n",
    "# checkpoint_path =  glob.glob(\"../saves/cca__roberta__idf__n_8__dropout_-1.0_1.0_0.0/*/*_770/checkpoints/last.ckpt\")[0]\n",
    "# checkpoint_path = glob.glob(\"../saves/cca__roberta__idf__n_8__dropout_-1.0_1.0_0.0/*/*_775/checkpoints/last.ckpt\")[0]\n",
    "checkpoint_path = glob.glob(\"../saves/cca__roberta__idf__n_16__dropout_-1.0_1.0_0.0/*/*_777/checkpoints/last.ckpt\")[0]\n",
    "\n",
    "model = ContrastiveCrossAttentionModel.load_from_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b1079e-107e-4da5-ac2b-9b941d1f0eda",
   "metadata": {},
   "source": [
    "## 2. Define attack in TextAttack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "996b9503-11dd-4378-bc97-794e195448be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textattack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9137c22-612e-44c4-8557-42dcdabd8fd5",
   "metadata": {},
   "source": [
    "### (a) Beam search + replace with `[MASK]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee1838b-420d-4fc0-a849-45afcf3f1b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<AttackedText \"<mask> my name is Jack\">,\n",
       " <AttackedText \"Hello <mask> name is Jack\">,\n",
       " <AttackedText \"Hello my <mask> is Jack\">,\n",
       " <AttackedText \"Hello my name <mask> Jack\">,\n",
       " <AttackedText \"Hello my name is <mask>\">]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WordSwapSingleWord(textattack.transformations.word_swap.WordSwap):\n",
    "    \"\"\"Takes a sentence and transforms it by replacing with a single fixed word.\n",
    "    \"\"\"\n",
    "    single_word: str\n",
    "    def __init__(self, single_word: str = \"?\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.single_word = single_word\n",
    "\n",
    "    def _get_replacement_words(self, _word: str):\n",
    "        return [self.single_word]\n",
    "\n",
    "transformation = WordSwapSingleWord(single_word=dm.document_tokenizer.mask_token)\n",
    "transformation(textattack.shared.AttackedText(\"Hello my name is Jack\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d1de22-75f2-4049-8cf8-6023ddf4403d",
   "metadata": {},
   "source": [
    "### (b) \"Attack success\" as fullfilment of the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d259f78c-2ece-4727-87e6-b7cb714e5afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "\n",
    "class ChangeClassificationToBelowTopKClasses(textattack.goal_functions.ClassificationGoalFunction):\n",
    "    k: int\n",
    "    def __init__(self, *args, k: int = 1, **kwargs):\n",
    "        self.k = k\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _is_goal_complete(self, model_output, _):\n",
    "        original_class_score = model_output[self.ground_truth_output]\n",
    "        num_better_classes = (model_output > original_class_score).sum()\n",
    "        return num_better_classes >= self.k\n",
    "\n",
    "    def _get_score(self, model_output, _):\n",
    "        return 1 - model_output[self.ground_truth_output]\n",
    "    \n",
    "    \n",
    "    \"\"\"have to reimplement the following method to change the precision on the sum-to-one condition.\"\"\"\n",
    "    def _process_model_outputs(self, inputs, scores):\n",
    "        \"\"\"Processes and validates a list of model outputs.\n",
    "        This is a task-dependent operation. For example, classification\n",
    "        outputs need to have a softmax applied.\n",
    "        \"\"\"\n",
    "        # Automatically cast a list or ndarray of predictions to a tensor.\n",
    "        if isinstance(scores, list):\n",
    "            scores = torch.tensor(scores)\n",
    "\n",
    "        # Ensure the returned value is now a tensor.\n",
    "        if not isinstance(scores, torch.Tensor):\n",
    "            raise TypeError(\n",
    "                \"Must have list, np.ndarray, or torch.Tensor of \"\n",
    "                f\"scores. Got type {type(scores)}\"\n",
    "            )\n",
    "\n",
    "        # Validation check on model score dimensions\n",
    "        if scores.ndim == 1:\n",
    "            # Unsqueeze prediction, if it's been squeezed by the model.\n",
    "            if len(inputs) == 1:\n",
    "                scores = scores.unsqueeze(dim=0)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Model return score of shape {scores.shape} for {len(inputs)} inputs.\"\n",
    "                )\n",
    "        elif scores.ndim != 2:\n",
    "            # If model somehow returns too may dimensions, throw an error.\n",
    "            raise ValueError(\n",
    "                f\"Model return score of shape {scores.shape} for {len(inputs)} inputs.\"\n",
    "            )\n",
    "        elif scores.shape[0] != len(inputs):\n",
    "            # If model returns an incorrect number of scores, throw an error.\n",
    "            raise ValueError(\n",
    "                f\"Model return score of shape {scores.shape} for {len(inputs)} inputs.\"\n",
    "            )\n",
    "        elif not ((scores.sum(dim=1) - 1).abs() < 1e-4).all():\n",
    "            # Values in each row should sum up to 1. The model should return a\n",
    "            # set of numbers corresponding to probabilities, which should add\n",
    "            # up to 1. Since they are `torch.float` values, allow a small\n",
    "            # error in the summation.\n",
    "            scores = torch.nn.functional.softmax(scores, dim=1)\n",
    "            if not ((scores.sum(dim=1) - 1).abs() < 1e-4).all():\n",
    "                raise ValueError(\"Model scores do not add up to 1.\")\n",
    "        return scores.cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe283b7-f7c1-40f8-ada4-bd3f695f8784",
   "metadata": {},
   "source": [
    "### (c) Wrapper for Wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21230a07-e9b6-4a20-8c5b-230b4ed2c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import datasets\n",
    "\n",
    "class WikiDataset(textattack.datasets.Dataset):\n",
    "    dataset: datasets.Dataset\n",
    "    most_recent_idx: Optional[int]\n",
    "    \n",
    "    def __init__(self, dm: WikipediaDataModule):\n",
    "        self.shuffled = True\n",
    "        self.dataset = [ex for ex in dm.val_dataset]\n",
    "        self.label_names = list(dm.val_dataset['name'])\n",
    "        self.most_recent_idx = None\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, i: int) -> Tuple[OrderedDict, int]:\n",
    "        input_dict = OrderedDict([\n",
    "            ('document', self.dataset[i]['document'])\n",
    "        ])\n",
    "        self.most_recent_idx = i\n",
    "        return input_dict, self.dataset[i]['text_key_id']\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb4405c-876a-4246-b33d-dbdfc80cd85c",
   "metadata": {},
   "source": [
    "## (d) Model wrapper that computes similarities of input documents with validation profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd35ad75-a681-4168-87b8-efee22ea41c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import transformers\n",
    "\n",
    "from model.model import Model\n",
    "\n",
    "class MyModelWrapper(textattack.models.wrappers.ModelWrapper):\n",
    "    model: Model\n",
    "    tokenizer: transformers.AutoTokenizer\n",
    "    max_seq_length: int\n",
    "    # Have to store dataset and example_index here sadly.\n",
    "    dataset: WikiDataset\n",
    "    num_nearest_neighbors: int\n",
    "    \n",
    "    def __init__(self, model: Model, wiki_dataset: WikiDataset, tokenizer: transformers.AutoTokenizer, max_seq_length: int = 128, num_nearest_neighbors: int = 16):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.wiki_dataset = wiki_dataset\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.num_nearest_neighbors = num_nearest_neighbors\n",
    "                 \n",
    "    def to(self, device):\n",
    "        self.model.to(device)\n",
    "        return self\n",
    "    \n",
    "    def _get_tokenized_profile(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        ex = self.wiki_dataset.dataset[idx]\n",
    "        out_ex = {}\n",
    "        for _k, _v in ex.items():\n",
    "            if _k.startswith('profile__'):\n",
    "                out_ex[_k.replace('profile__', '')] = torch.tensor([_v])\n",
    "        return out_ex\n",
    "\n",
    "    def _get_nearest_neighbors_tokenized(self) -> Tuple[List[int], Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Gets the nearest-neighbors of an example. Used for contrastive learning.\"\"\"\n",
    "        ex = self.wiki_dataset.dataset[self.wiki_dataset.most_recent_idx]\n",
    "        assert \"nearest_neighbor_idxs\" in ex\n",
    "        out_ex = {}\n",
    "        eligible_neighbor_idxs = [\n",
    "            _i for _i in ex[\"nearest_neighbor_idxs\"]\n",
    "        ]\n",
    "        assert len(eligible_neighbor_idxs) >= self.num_nearest_neighbors\n",
    "        neighbor_idxs = eligible_neighbor_idxs[:self.num_nearest_neighbors]\n",
    "\n",
    "        neighbors_tokenized = [\n",
    "           self._get_tokenized_profile(idx=n_idx) for n_idx in neighbor_idxs if n_idx < len(self.wiki_dataset.dataset)\n",
    "        ]\n",
    "        keys = neighbors_tokenized[0].keys() # probably like {'input_ids', 'attention_mask'}\n",
    "        for _k in keys:\n",
    "            out_ex[_k] = torch.stack([\n",
    "                n[_k][0] for n in neighbors_tokenized\n",
    "            ])\n",
    "        return neighbor_idxs, out_ex\n",
    "\n",
    "    def __call__(self, text_input_list: List[str], batch_size=32):\n",
    "        model_device = next(self.model.parameters()).device\n",
    "        # TODO: Use bm25 on first input to get neighbor IDXs?\n",
    "        \n",
    "        # Tokenize document\n",
    "        doc_inputs = self.tokenizer.batch_encode_plus(\n",
    "            text_input_list,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        doc_inputs = {k: v for k,v in doc_inputs.items()}\n",
    "        # print('doc_inputs.', doc_inputs['input_ids'].shape)\n",
    "        \n",
    "        # Get neighboring profiles\n",
    "        neighbor_idxs, profile_inputs = self._get_nearest_neighbors_tokenized()\n",
    "        num_neighbors = len(profile_inputs['input_ids'])\n",
    "        \n",
    "        # Do a loop. TODO: use repeats/tensors\n",
    "        outputs = np.zeros((len(text_input_list), len(self.wiki_dataset.dataset)), dtype=np.float32)\n",
    "        for idx in range(len(text_input_list)):\n",
    "            doc_inputs_idx = { k: v[idx].repeat((num_neighbors, 1)) for k, v in doc_inputs.items() }\n",
    "            # print('doc_inputs_idx', doc_inputs_idx['input_ids'].shape)\n",
    "            # print('profile_inputs', profile_inputs['input_ids'].shape)\n",
    "            all_inputs = {\n",
    "                key: torch.cat((doc_inputs_idx[key], profile_inputs[key]), dim=1).to(model_device)\n",
    "                for key in doc_inputs\n",
    "            }\n",
    "            with torch.no_grad():\n",
    "                outputs_idx = self.model.forward_document_and_profile_inputs(inputs=all_inputs).squeeze()\n",
    "            outputs[idx, neighbor_idxs] = outputs_idx.softmax(dim=0).cpu()\n",
    "        return np.vstack(outputs)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8767075b-0bdf-4572-9b48-9e801572c221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MyModelWrapper at 0x7ff5bcdc05b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_dataset = WikiDataset(dm)\n",
    "model_wrapper = MyModelWrapper(model=model, wiki_dataset=wiki_dataset, num_nearest_neighbors=16, tokenizer=dm.document_tokenizer, max_seq_length=dm.max_seq_length)\n",
    "model_wrapper.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0224b24d-10e0-4bfb-b841-1eaa8c8fd4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malcolm_mackey = dm.val_dataset[101]['document']\n",
    "wiki_dataset[101] # access sets most_recent_idx\n",
    "model_wrapper([malcolm_mackey]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13d64ec5-781b-4ef7-a08c-7c252db3740c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** printing nearest-neighbors **\n",
      "\t\t100.00% \t Anthony Acid\n",
      "\t\t0.00% \t Dj Edwin\n",
      "\t\t0.00% \t Dave Clarke -lrb- Techno Dj -rrb-\n",
      "\t\t0.00% \t Dj Bobo\n",
      "\t\t0.00% \t Dj Roxxi\n",
      "\t\t0.00% \t Pmd\n",
      "\t\t0.00% \t Anthony Johnson\n",
      "\t\t0.00% \t Mclean\n",
      "\t\t0.00% \t Anthony Caruso\n",
      "\t\t0.00% \t 3rd Bass\n",
      "\t\t0.00% \t Anthony Stacchi\n",
      "\t\t0.00% \t Drew Parks\n",
      "\t\t0.00% \t Otto Knows\n",
      "\t\t0.00% \t Lenny Fontana\n",
      "\t\t0.00% \t Mike Dierickx\n",
      "\t\t0.00% \t Amin Golestan Parast\n"
     ]
    }
   ],
   "source": [
    "idx = 50\n",
    "\n",
    "ex = dm.val_dataset[idx]\n",
    "doc = ex['document']\n",
    "wiki_dataset[idx] # access sets most_recent_idx\n",
    "all_scores = model_wrapper([doc])\n",
    "scores = all_scores[0, ex['nearest_neighbor_idxs']]\n",
    "\n",
    "print('** printing nearest-neighbors **')\n",
    "for idx, score in zip(model_wrapper.wiki_dataset.dataset[idx]['nearest_neighbor_idxs'][:16], scores.tolist()):\n",
    "    print(f'\\t\\t{score*100:.2f}%', '\\t',model_wrapper.wiki_dataset.dataset[idx]['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abde44f-912e-4f31-8abd-8706ce99bbc6",
   "metadata": {},
   "source": [
    "## 3. Run attack once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f77f286-3f48-44b5-b341-35582807ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxNumWordsModified(textattack.constraints.PreTransformationConstraint):\n",
    "    def __init__(self, max_num_words: int):\n",
    "        self.max_num_words = max_num_words\n",
    "\n",
    "    def _get_modifiable_indices(self, current_text):\n",
    "        \"\"\"Returns the word indices in current_text which are able to be\n",
    "        modified.\"\"\"\n",
    "\n",
    "        if len(current_text.attack_attrs[\"modified_indices\"]) >= self.max_num_words:\n",
    "            return set()\n",
    "        else:\n",
    "            return set(range(len(current_text.words)))\n",
    "\n",
    "    def extra_repr_keys(self):\n",
    "        return [\"max_num_words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31eea9ed-d889-4d57-a763-50b06aaf2a20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textattack.shared import utils\n",
    "\n",
    "\n",
    "def get_modified_idxs_in_order(at: textattack.shared.AttackedText) -> List[int]:\n",
    "    \"\"\"Traverses linked-list of attacked texts from attack process\n",
    "    and creates a list of the modified word indices.\n",
    "    \"\"\"\n",
    "    modified_word_idxs = []\n",
    "    while True:\n",
    "        if 'newly_modified_indices' not in at.attack_attrs:\n",
    "            break\n",
    "        modified_word_idxs.extend(at.attack_attrs['newly_modified_indices'])\n",
    "        at = at.attack_attrs['prev_attacked_text']\n",
    "    modified_word_idxs = modified_word_idxs[::-1]\n",
    "    return modified_word_idxs[::-1]\n",
    "\n",
    "\n",
    "def diff_color_with_idxs(at: textattack.attack_results.AttackResult, color_method=None):\n",
    "    \"\"\"Highlights the difference between two texts using color.\n",
    "    \n",
    "    This version also adds idx numbers to show which words were masked in which order.\n",
    "\n",
    "    Has to account for deletions and insertions from original text to\n",
    "    perturbed. Relies on the index map stored in\n",
    "    ``self.original_result.attacked_text.attack_attrs[\"original_index_map\"]``.\n",
    "    \"\"\"\n",
    "    t1 = at.original_result.attacked_text\n",
    "    t2 = at.perturbed_result.attacked_text\n",
    "\n",
    "    if color_method is None:\n",
    "        return t1.printable_text(), t2.printable_text()\n",
    "\n",
    "    color_1 = at.original_result.get_text_color_input()\n",
    "    color_2 = at.perturbed_result.get_text_color_perturbed()\n",
    "\n",
    "    # iterate through and count equal/unequal words\n",
    "    words_1_idxs = []\n",
    "    t2_equal_idxs = set()\n",
    "    original_index_map = t2.attack_attrs[\"original_index_map\"]\n",
    "    for t1_idx, t2_idx in enumerate(original_index_map):\n",
    "        if t2_idx == -1:\n",
    "            # add words in t1 that are not in t2\n",
    "            words_1_idxs.append(t1_idx)\n",
    "        else:\n",
    "            w1 = t1.words[t1_idx]\n",
    "            w2 = t2.words[t2_idx]\n",
    "            if w1 == w2:\n",
    "                t2_equal_idxs.add(t2_idx)\n",
    "            else:\n",
    "                words_1_idxs.append(t1_idx)\n",
    "\n",
    "    # words to color in t2 are all the words that didn't have an equal,\n",
    "    # mapped word in t1\n",
    "    words_2_idxs = list(sorted(set(range(t2.num_words)) - t2_equal_idxs))\n",
    "\n",
    "    # make lists of colored words\n",
    "    words_1 = [t1.words[i] for i in words_1_idxs]\n",
    "    words_1 = [utils.color_text(w, color_1, color_method) for w in words_1]\n",
    "    \n",
    "    # First, replace words with `word_xx` where xx is the index\n",
    "    # of the order that word was modified.\n",
    "    word_modification_order = {word_idx: swap_idx+1 for swap_idx, word_idx in enumerate(get_modified_idxs_in_order(t2))}\n",
    "    words_2 = [f'{t2.words[i]}__{word_modification_order[i]}' for i in words_2_idxs]\n",
    "    words_2 = [utils.color_text(w, color_2, color_method) for w in words_2]\n",
    "\n",
    "    t1 = at.original_result.attacked_text.replace_words_at_indices(\n",
    "        words_1_idxs, words_1\n",
    "    )\n",
    "    t2 = at.perturbed_result.attacked_text.replace_words_at_indices(\n",
    "        words_2_idxs, words_2\n",
    "    )\n",
    "\n",
    "    key_color = (\"bold\", \"underline\")\n",
    "    return (\n",
    "        t1.printable_text(key_color=key_color, key_color_method=color_method),\n",
    "        t2.printable_text(key_color=key_color, key_color_method=color_method),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a665c63-5457-4740-923c-9178fe185729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textattack.loggers import CSVLogger\n",
    "from textattack.shared import AttackedText\n",
    "\n",
    "import pandas as pd\n",
    "class CustomCSVLogger(CSVLogger):\n",
    "    \"\"\"Logs attack results to a CSV.\"\"\"\n",
    "\n",
    "    def log_attack_result(self, result: textattack.goal_function_results.ClassificationGoalFunctionResult):\n",
    "        # TODO print like 'mask1', 'mask2',\n",
    "        original_text, perturbed_text = diff_color_with_idxs(result, color_method=self.color_method)\n",
    "        original_text = original_text.replace(\"\\n\", AttackedText.SPLIT_TOKEN)\n",
    "        perturbed_text = perturbed_text.replace(\"\\n\", AttackedText.SPLIT_TOKEN)\n",
    "        result_type = result.__class__.__name__.replace(\"AttackResult\", \"\")\n",
    "        row = {\n",
    "            \"original_person\": result.original_result._processed_output[0],\n",
    "            \"original_text\": original_text,\n",
    "            \"original_text_id_bm25\": bm25.get_scores(result.original_result.attacked_text.text.split()).argmax(),\n",
    "            \"perturbed_person\": result.perturbed_result._processed_output[0],\n",
    "            \"perturbed_text\": perturbed_text,\n",
    "            \"perturbed_text_id_bm25\": bm25.get_scores(result.perturbed_result.attacked_text.text.split()).argmax(),\n",
    "            \"original_score\": result.original_result.score,\n",
    "            \"perturbed_score\": result.perturbed_result.score,\n",
    "            \"original_output\": result.original_result.output,\n",
    "            \"perturbed_output\": result.perturbed_result.output,\n",
    "            \"ground_truth_output\": result.original_result.ground_truth_output,\n",
    "            \"num_queries\": result.num_queries,\n",
    "            \"result_type\": result_type,\n",
    "        }\n",
    "        self.df = pd.concat([self.df, pd.DataFrame([row])], ignore_index=True)\n",
    "        self._flushed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46ac1f10-811c-47b5-9bd3-f2f92a0f2183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-b236d6d839d77887.arrow\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def get_words_from_doc(s: List[str]) -> List[str]:\n",
    "    words = s.split()\n",
    "    return [w for w in words if not w in eng_stopwords]\n",
    "\n",
    "def make_table_str(ex):\n",
    "    ex['table_str'] = (\n",
    "        ' '.join(ex['input_text']['table']['column_header'] + ex['input_text']['table']['content'])\n",
    "    )\n",
    "    return ex\n",
    "\n",
    "prof_data = dm.val_dataset.map(make_table_str)\n",
    "profile_corpus = prof_data['table_str']\n",
    "\n",
    "tokenized_profile_corpus = [\n",
    "    get_words_from_doc(prof) for prof in profile_corpus\n",
    "]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_profile_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5e37aa2-e2fb-476a-b8e5-83e2d8709a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MyModelWrapper at 0x7ff3e981d0a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_dataset = WikiDataset(dm)\n",
    "model_wrapper = MyModelWrapper(model=model, wiki_dataset=wiki_dataset, num_nearest_neighbors=16, tokenizer=dm.document_tokenizer, max_seq_length=dm.max_seq_length)\n",
    "model_wrapper.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16ff1b8e-7c8e-47a8-b803-51075ed032ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: No entry found for goal function <class '__main__.ChangeClassificationToBelowTopKClasses'>.\n",
      "textattack: Unknown if model of class <class 'model.contrastive_cross_attention.ContrastiveCrossAttentionModel'> compatible with goal function <class '__main__.ChangeClassificationToBelowTopKClasses'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): BeamSearch(\n",
      "    (beam_width):  2\n",
      "  )\n",
      "  (goal_function):  ChangeClassificationToBelowTopKClasses\n",
      "  (transformation):  WordSwapSingleWord\n",
      "  (constraints): \n",
      "    (0): RepeatModification\n",
      "    (1): MaxWordIndexModification(\n",
      "        (max_length):  128\n",
      "      )\n",
      "    (2): MaxNumWordsModified(\n",
      "        (max_num_words):  50\n",
      "      )\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 8 / 2 / 0 / 10: 100%|██████████| 10/10 [05:11<00:00, 31.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 8      |\n",
      "| Number of failed attacks:     | 2      |\n",
      "| Number of skipped attacks:    | 0      |\n",
      "| Original accuracy:            | 100.0% |\n",
      "| Accuracy under attack:        | 20.0%  |\n",
      "| Attack success rate:          | 80.0%  |\n",
      "| Average perturbed word %:     | 40.79% |\n",
      "| Average num. words per input: | 37.1   |\n",
      "| Avg num queries:              | 746.1  |\n",
      "+-------------------------------+--------+"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "textattack: Logging to CSV at path results.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_person</th>\n",
       "      <th>original_text</th>\n",
       "      <th>original_text_id_bm25</th>\n",
       "      <th>perturbed_person</th>\n",
       "      <th>perturbed_text</th>\n",
       "      <th>perturbed_text_id_bm25</th>\n",
       "      <th>original_score</th>\n",
       "      <th>perturbed_score</th>\n",
       "      <th>original_output</th>\n",
       "      <th>perturbed_output</th>\n",
       "      <th>ground_truth_output</th>\n",
       "      <th>num_queries</th>\n",
       "      <th>result_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Michael iii of alexandria</td>\n",
       "      <td>pope <font color = red>michael</font> <font color = red>iii</font> of alexandria ( also known as khail iii ) was the coptic pope of alexandria and patriarch of the see of st. mark ( <font color = red>880</font> -- <font color = red>907</font> ) .<SPLIT>in 882 , the governor of egypt , ahmad ibn tulun , forced khail to pay heavy contributions , forcing him to sell a church and some attached properties to the local jewish community .<SPLIT>this building was at one time believed to have later become the site of the cairo geniza .<SPLIT></td>\n",
       "      <td>0</td>\n",
       "      <td>Michael iv of alexandria</td>\n",
       "      <td>pope <<font color = red>mask__1</font>> <<font color = red>mask__4</font>> of alexandria ( also known as khail iii ) was the coptic pope of alexandria and patriarch of the see of st. mark ( <<font color = red>mask__2</font>> -- <<font color = red>mask__3</font>> ) .<SPLIT>in 882 , the governor of egypt , ahmad ibn tulun , forced khail to pay heavy contributions , forcing him to sell a church and some attached properties to the local jewish community .<SPLIT>this building was at one time believed to have later become the site of the cairo geniza .<SPLIT></td>\n",
       "      <td>0</td>\n",
       "      <td>0.022335</td>\n",
       "      <td>0.825860</td>\n",
       "      <td>0</td>\n",
       "      <td>12300</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>Successful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hui jun</td>\n",
       "      <td><font color = green>hui</font> <font color = green>jun</font> <font color = green>is</font> <font color = green>a</font> <font color = green>male</font> <font color = green>former</font> <font color = green>table</font> <font color = green>tennis</font> <font color = green>player</font> <font color = green>from</font> <font color = green>china</font> .<SPLIT></td>\n",
       "      <td>1</td>\n",
       "      <td>Jun marques davidson</td>\n",
       "      <td><<font color = purple>mask__11</font>> <<font color = purple>mask__10</font>> <<font color = purple>mask__6</font>> <<font color = purple>mask__5</font>> <<font color = purple>mask__2</font>> <<font color = purple>mask__9</font>> <<font color = purple>mask__3</font>> <<font color = purple>mask__4</font>> <<font color = purple>mask__7</font>> <<font color = purple>mask__1</font>> <<font color = purple>mask__8</font>> .<SPLIT></td>\n",
       "      <td>2433</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.763827</td>\n",
       "      <td>1</td>\n",
       "      <td>4393</td>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>Successful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Okan öztürk</td>\n",
       "      <td><font color = blue>okan</font> <font color = blue>Öztürk</font> ( <font color = blue>born</font> <font color = blue>30</font> <font color = blue>november</font> <font color = blue>1977</font> ) <font color = blue>is</font> <font color = blue>a</font> <font color = blue>turkish</font> <font color = blue>professional</font> <font color = blue>footballer</font> .<SPLIT><font color = blue>he</font> <font color = blue>currently</font> <font color = blue>plays</font> <font color = blue>as</font> <font color = blue>a</font> <font color = blue>striker</font> <font color = blue>for</font> <font color = blue>yeni</font> <font color = blue>malatyaspor</font> .<SPLIT></td>\n",
       "      <td>2</td>\n",
       "      <td>Okan öztürk</td>\n",
       "      <td><<font color = blue>mask__19</font>> <<font color = blue>mask__20</font>> ( <<font color = blue>mask__3</font>> <<font color = blue>mask__2</font>> <<font color = blue>mask__1</font>> <<font color = blue>mask__18</font>> ) <<font color = blue>mask__6</font>> <<font color = blue>mask__7</font>> <<font color = blue>mask__14</font>> <<font color = blue>mask__4</font>> <<font color = blue>mask__13</font>> .<SPLIT><<font color = blue>mask__5</font>> <<font color = blue>mask__11</font>> <<font color = blue>mask__10</font>> <<font color = blue>mask__8</font>> <<font color = blue>mask__12</font>> <<font color = blue>mask__17</font>> <<font color = blue>mask__9</font>> <<font color = blue>mask__16</font>> <<font color = blue>mask__15</font>> .<SPLIT></td>\n",
       "      <td>2433</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.079084</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>401</td>\n",
       "      <td>Failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marie stephan</td>\n",
       "      <td><font color = purple>marie</font> <font color = purple>stephan</font> , ( born <font color = purple>march</font> <font color = purple>14</font> , <font color = purple>1996</font> ) is a professional squash player who represents france .<SPLIT><font color = purple>she</font> reached a career-high world ranking of world no. <font color = purple>101</font> in <font color = purple>july</font> <font color = purple>2015</font> .<SPLIT></td>\n",
       "      <td>3</td>\n",
       "      <td>Laura pomportes</td>\n",
       "      <td><<font color = pink>mask__2</font>> <<font color = pink>mask__3</font>> , ( born <<font color = pink>mask__1</font>> <<font color = pink>mask__4</font>> , <<font color = pink>mask__5</font>> ) is a professional squash player who represents france .<SPLIT><<font color = pink>mask__8</font>> reached a career-high world ranking of world no. <<font color = pink>mask__7</font>> in <<font color = pink>mask__9</font>> <<font color = pink>mask__6</font>> .<SPLIT></td>\n",
       "      <td>4624</td>\n",
       "      <td>0.004798</td>\n",
       "      <td>0.730643</td>\n",
       "      <td>3</td>\n",
       "      <td>4726</td>\n",
       "      <td>3</td>\n",
       "      <td>388</td>\n",
       "      <td>Successful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leonard l. martino</td>\n",
       "      <td><font color = yellow>leonard</font> l. <font color = yellow>martino</font> <font color = yellow>is</font> <font color = yellow>a</font> former <font color = yellow>democratic</font> <font color = yellow>member</font> <font color = yellow>of</font> the <font color = yellow>pennsylvania</font> <font color = yellow>house</font> of <font color = yellow>representatives</font> .<SPLIT>he was born in <font color = yellow>butler</font> to michael and angela pitullio <font color = yellow>martino</font> .<SPLIT></td>\n",
       "      <td>4</td>\n",
       "      <td>Howard l. morgan</td>\n",
       "      <td><<font color = brown>mask__12</font>> l. <<font color = brown>mask__11</font>> <<font color = brown>mask__1</font>> <<font color = brown>mask__2</font>> former <<font color = brown>mask__5</font>> <<font color = brown>mask__9</font>> <<font color = brown>mask__4</font>> the <<font color = brown>mask__7</font>> <<font color = brown>mask__6</font>> of <<font color = brown>mask__3</font>> .<SPLIT>he was born in <<font color = brown>mask__10</font>> to michael and angela pitullio <<font color = brown>mask__8</font>> .<SPLIT></td>\n",
       "      <td>10833</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.544473</td>\n",
       "      <td>4</td>\n",
       "      <td>6629</td>\n",
       "      <td>4</td>\n",
       "      <td>444</td>\n",
       "      <td>Successful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Salome jens</td>\n",
       "      <td><font color = orange>salome</font> <font color = orange>jens</font> ( <font color = orange>born</font> <font color = orange>may</font> <font color = orange>8</font> , <font color = orange>1935</font> ) <font color = orange>is</font> <font color = orange>an</font> <font color = orange>american</font> <font color = orange>stage</font> , <font color = orange>film</font> <font color = orange>and</font> <font color = orange>television</font> <font color = orange>actress</font> .<SPLIT><font color = orange>she</font> <font color = orange>is</font> <font color = orange>perhaps</font> <font color = orange>best</font> <font color = orange>known</font> <font color = orange>for</font> <font color = orange>portraying</font> <font color = orange>the</font> <font color = orange>female</font> <font color = orange>changeling</font> <font color = orange>on</font> '' '' .<SPLIT></td>\n",
       "      <td>5</td>\n",
       "      <td>Salome jens</td>\n",
       "      <td><<font color = orange>mask__20</font>> <<font color = orange>mask__23</font>> ( <<font color = orange>mask__8</font>> <<font color = orange>mask__22</font>> <<font color = orange>mask__24</font>> , <<font color = orange>mask__21</font>> ) <<font color = orange>mask__18</font>> <<font color = orange>mask__19</font>> <<font color = orange>mask__5</font>> <<font color = orange>mask__11</font>> , <<font color = orange>mask__17</font>> <<font color = orange>mask__10</font>> <<font color = orange>mask__13</font>> <<font color = orange>mask__1</font>> .<SPLIT><<font color = orange>mask__15</font>> <<font color = orange>mask__14</font>> <<font color = orange>mask__7</font>> <<font color = orange>mask__6</font>> <<font color = orange>mask__2</font>> <<font color = orange>mask__4</font>> <<font color = orange>mask__16</font>> <<font color = orange>mask__12</font>> <<font color = orange>mask__3</font>> <<font color = orange>mask__25</font>> <<font color = orange>mask__9</font>> '' '' .<SPLIT></td>\n",
       "      <td>11804</td>\n",
       "      <td>0.032871</td>\n",
       "      <td>0.236834</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>626</td>\n",
       "      <td>Failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Carl crawford</td>\n",
       "      <td><font color = pink>carl</font> <font color = pink>demonte</font> <font color = pink>crawford</font> ( born august 5 , 1981 ) , nicknamed `` the perfect storm '' , is an american professional baseball <font color = pink>left</font> <font color = pink>fielder</font> <font color = pink>with</font> the <font color = pink>los</font> <font color = pink>angeles</font> <font color = pink>dodgers</font> of major league baseball ( mlb ) .<SPLIT>he bats and throws <font color = pink>left-handed</font> .<SPLIT><font color = pink>crawford</font> was drafted by the tampa bay <font color = pink>devil</font> rays in the second round ( 52nd overall ) of the <font color = pink>1999</font> major league baseball draft .<SPLIT>he made his major league debut in <font color = pink>2002</font> .<SPLIT><font color = pink>crawford</font> has more <font color = pink>triples</font> ( <font color = pink>121</font> ) than any other active baseball player .<SPLIT></td>\n",
       "      <td>6</td>\n",
       "      <td>Wade davis</td>\n",
       "      <td><<font color = red>mask__15</font>> <<font color = red>mask__16</font>> <<font color = red>mask__17</font>> ( born august 5 , 1981 ) , nicknamed `` the perfect storm '' , is an american professional baseball <<font color = red>mask__7</font>> <<font color = red>mask__14</font>> <<font color = red>mask__4</font>> the <<font color = red>mask__11</font>> <<font color = red>mask__2</font>> <<font color = red>mask__1</font>> of major league baseball ( mlb ) .<SPLIT>he bats and throws <<font color = red>mask__5</font>> .<SPLIT><<font color = red>mask__9</font>> was drafted by the tampa bay <<font color = red>mask__3</font>> rays in the second round ( 52nd overall ) of the <<font color = red>mask__12</font>> major league baseball draft .<SPLIT>he made his major league debut in <<font color = red>mask__13</font>> .<SPLIT><<font color = red>mask__10</font>> has more <<font color = red>mask__8</font>> ( <<font color = red>mask__6</font>> ) than any other active baseball player .<SPLIT></td>\n",
       "      <td>9148</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.696510</td>\n",
       "      <td>6</td>\n",
       "      <td>8580</td>\n",
       "      <td>6</td>\n",
       "      <td>2204</td>\n",
       "      <td>Successful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jim bob</td>\n",
       "      <td><font color = cyan>jim</font> <font color = cyan>bob</font> ( <font color = cyan>born</font> james <font color = cyan>neil</font> <font color = cyan>morrison</font> on <font color = cyan>22</font> <font color = cyan>november</font> <font color = cyan>1960</font> ) is a british <font color = cyan>musician</font> and author , best known as the <font color = cyan>singer</font> of indie punk band <font color = cyan>carter</font> usm .<SPLIT></td>\n",
       "      <td>7</td>\n",
       "      <td>The lord lloyd-webber</td>\n",
       "      <td><<font color = green>mask__2</font>> <<font color = green>mask__4</font>> ( <<font color = green>mask__1</font>> james <<font color = green>mask__9</font>> <<font color = green>mask__7</font>> on <<font color = green>mask__3</font>> <<font color = green>mask__10</font>> <<font color = green>mask__5</font>> ) is a british <<font color = green>mask__11</font>> and author , best known as the <<font color = green>mask__6</font>> of indie punk band <<font color = green>mask__8</font>> usm .<SPLIT></td>\n",
       "      <td>4953</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.805126</td>\n",
       "      <td>7</td>\n",
       "      <td>12691</td>\n",
       "      <td>7</td>\n",
       "      <td>458</td>\n",
       "      <td>Successful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Riddick parker</td>\n",
       "      <td><font color = gray>riddick</font> <font color = gray>parker</font> ( born <font color = gray>november</font> <font color = gray>20</font> , <font color = gray>1972</font> <font color = gray>in</font> <font color = gray>emporia</font> , <font color = gray>virginia</font> ) is a former professional american football defensive <font color = gray>lineman</font> for the <font color = gray>seattle</font> <font color = gray>seahawks</font> , san diego chargers , new england patriots , <font color = gray>baltimore</font> <font color = gray>ravens</font> , <font color = gray>and</font> san <font color = gray>francisco</font> <font color = gray>49ers</font> of the national football league .<SPLIT></td>\n",
       "      <td>8</td>\n",
       "      <td>Gene huey</td>\n",
       "      <td><<font color = gray>mask__16</font>> <<font color = gray>mask__15</font>> ( born <<font color = gray>mask__11</font>> <<font color = gray>mask__3</font>> , <<font color = gray>mask__12</font>> <<font color = gray>mask__1</font>> <<font color = gray>mask__13</font>> , <<font color = gray>mask__10</font>> ) is a former professional american football defensive <<font color = gray>mask__14</font>> for the <<font color = gray>mask__9</font>> <<font color = gray>mask__8</font>> , san diego chargers , new england patriots , <<font color = gray>mask__7</font>> <<font color = gray>mask__6</font>> , <<font color = gray>mask__2</font>> san <<font color = gray>mask__5</font>> <<font color = gray>mask__4</font>> of the national football league .<SPLIT></td>\n",
       "      <td>5946</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.612135</td>\n",
       "      <td>8</td>\n",
       "      <td>5658</td>\n",
       "      <td>8</td>\n",
       "      <td>939</td>\n",
       "      <td>Successful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Blessed osanna of cattaro -lrb- ozana kotorska -rrb-</td>\n",
       "      <td><font color = brown>blessed</font> <font color = brown>osanna</font> <font color = brown>of</font> <font color = brown>cattaro</font> <font color = brown>t</font>.<font color = brown>o</font>.<font color = brown>s</font>.<font color = brown>d</font>. ( ) <font color = brown>was</font> <font color = brown>a</font> catholic <font color = brown>visionary</font> and anchoress from <font color = brown>cattaro</font> ( kotor ) .<SPLIT>she was a teenage convert from orthodoxy <font color = brown>of</font> serbian descent from <font color = brown>montenegro</font> ( zeta ) .<SPLIT>she became a dominican tertiary and was posthumously venerated <font color = brown>as</font> a saint in kotor .<SPLIT>she was <font color = brown>later</font> beatified in <font color = brown>1934</font> .<SPLIT></td>\n",
       "      <td>9</td>\n",
       "      <td>Florentina of cartagena</td>\n",
       "      <td><<font color = green>mask__14</font>> <<font color = green>mask__17</font>> <<font color = green>mask__12</font>> <<font color = green>mask__16</font>> <<font color = green>mask__6</font>>.<<font color = green>mask__5</font>>.<<font color = green>mask__2</font>>.<<font color = green>mask__10</font>>. ( ) <<font color = green>mask__8</font>> <<font color = green>mask__3</font>> catholic <<font color = green>mask__7</font>> and anchoress from <<font color = green>mask__15</font>> ( kotor ) .<SPLIT>she was a teenage convert from orthodoxy <<font color = green>mask__11</font>> serbian descent from <<font color = green>mask__9</font>> ( zeta ) .<SPLIT>she became a dominican tertiary and was posthumously venerated <<font color = green>mask__4</font>> a saint in kotor .<SPLIT>she was <<font color = green>mask__1</font>> beatified in <<font color = green>mask__13</font>> .<SPLIT></td>\n",
       "      <td>9</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.601613</td>\n",
       "      <td>9</td>\n",
       "      <td>12171</td>\n",
       "      <td>9</td>\n",
       "      <td>1379</td>\n",
       "      <td>Successful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \n",
    "#  Initialize attack\n",
    "# \n",
    "\n",
    "from textattack import Attack\n",
    "from textattack.constraints.pre_transformation import MaxWordIndexModification, RepeatModification\n",
    "\n",
    "goal_function = ChangeClassificationToBelowTopKClasses(model_wrapper, k=1)\n",
    "constraints = [\n",
    "    RepeatModification(),\n",
    "    MaxWordIndexModification(max_length=dm.max_seq_length),\n",
    "    MaxNumWordsModified(max_num_words=50)\n",
    "]\n",
    "transformation = WordSwapSingleWord(single_word=dm.document_tokenizer.mask_token)\n",
    "search_method = textattack.search_methods.BeamSearch(beam_width=2)\n",
    "\n",
    "attack = Attack(\n",
    "    goal_function, constraints, transformation, search_method\n",
    ")\n",
    "\n",
    "from tqdm import tqdm # tqdm provides us a nice progress bar.\n",
    "from textattack.attack_results import SuccessfulAttackResult\n",
    "from textattack import Attacker\n",
    "from textattack import AttackArgs\n",
    "\n",
    "attack_args = AttackArgs(num_examples=10, disable_stdout=True)\n",
    "\n",
    "attacker = Attacker(attack, wiki_dataset, attack_args)\n",
    "\n",
    "results_iterable = attacker.attack_dataset()\n",
    "\n",
    "logger = CustomCSVLogger(color_method='html')\n",
    "\n",
    "# \n",
    "# Run attack\n",
    "# \n",
    "from tqdm import tqdm\n",
    "\n",
    "for result in results_iterable:\n",
    "    tqdm._instances.clear() # Doesn't fix the progress bar :-(\n",
    "    logger.log_attack_result(result)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(logger.df.to_html(escape=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad4564-e2be-40d2-8795-223c8e650587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
