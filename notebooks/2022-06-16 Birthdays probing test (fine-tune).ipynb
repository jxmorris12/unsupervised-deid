{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54dd8d98-0748-478b-8c82-6b77cfa44e53",
   "metadata": {},
   "source": [
    "# Birthdays probing test - finetuning\n",
    "\n",
    "what if we *fine-tune* distilbert on the birthday task? what's its performance look like then?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d252cb9-4ee5-42f2-98df-af6e72555090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jxm3/research/deidentification/unsupervised-deidentification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "529344f2-556a-49e3-92e4-b5055c9b0469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DocumentProfileMatchingTransformer with learning_rate = 1e-06\n"
     ]
    }
   ],
   "source": [
    "from model import DocumentProfileMatchingTransformer\n",
    "\n",
    "import os\n",
    "\n",
    "num_cpus = os.cpu_count()\n",
    "\n",
    "model = DocumentProfileMatchingTransformer(\n",
    "    document_model_name_or_path='roberta-base',\n",
    "    profile_model_name_or_path='distilbert-base-uncased',\n",
    "    num_workers=min(8, num_cpus),\n",
    "    train_batch_size=64,\n",
    "    eval_batch_size=64,\n",
    "    learning_rate=1e-6,\n",
    "    max_seq_length=256,\n",
    "    pretrained_profile_encoder=False,\n",
    "    word_dropout_ratio=0.0,\n",
    "    word_dropout_perc=0.0,\n",
    "    lr_scheduler_factor=0.5,\n",
    "    lr_scheduler_patience=3,\n",
    "    adversarial_mask_k_tokens=0,\n",
    "    train_without_names=False,\n",
    ")\n",
    "del model.document_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29132408-7f12-472e-896b-494b99fda702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing WikipediaDataModule with num_workers = 8 and mask token `<mask>`\n",
      "loading wiki_bio[1.2.0] split train[:100%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_bio (/home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading wiki_bio[1.2.0] split val[:20%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_bio (/home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da)\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-58e5e96e220311ed.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-778e9a6d1b0dfab7.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-3c4e94260fbd4dd3.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-9e279afc7bfb46f2.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-7c5ac0e6f364c103.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-02418e1d9ade71ab.arrow\n"
     ]
    }
   ],
   "source": [
    "from dataloader import WikipediaDataModule\n",
    "import os\n",
    "\n",
    "num_cpus = os.cpu_count()\n",
    "\n",
    "dm = WikipediaDataModule(\n",
    "    mask_token=model.document_tokenizer.mask_token,\n",
    "    dataset_name='wiki_bio',\n",
    "    dataset_train_split='train[:100%]',\n",
    "    dataset_val_split='val[:20%]',\n",
    "    dataset_version='1.2.0',\n",
    "    num_workers=min(8, num_cpus),\n",
    "    train_batch_size=64,\n",
    "    eval_batch_size=64,\n",
    ")\n",
    "dm.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ea5280-96de-48cd-9ccc-23d8c3f3237d",
   "metadata": {},
   "source": [
    "## Get the birthday data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "774c9b7c-aedb-4ea6-9635-4c9ddd5c0639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "d = datetime.datetime.strptime('17 january 1943', \"%d %B %Y\")\n",
    "d.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c6da6f-9788-4a13-bb05-46383951d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "\n",
    "def process_dataset(_dataset) -> List[Tuple[int, int]]:\n",
    "    _processed_data = []\n",
    "    for idx, d in enumerate(tqdm(_dataset, 'processing birthdays')):\n",
    "        profile = d['profile']\n",
    "        date_str_matches = re.search(r\"birth_date \\| ([\\d]{1,4} [a-z]+ [\\d]{1,4})\", profile)\n",
    "        if date_str_matches:\n",
    "            date_str = date_str_matches.group(1)\n",
    "            # print(date_str)\n",
    "            # parse to datetime.datetime\n",
    "            try:\n",
    "                dt = datetime.datetime.strptime(date_str, \"%d %B %Y\")\n",
    "            except ValueError as e:\n",
    "                # print(e)\n",
    "                continue\n",
    "            # day_class_num = (dt.month - 1) * 31 + (dt.day - 1)\n",
    "            # _processed_data.append((idx, day_class_num))\n",
    "            _processed_data.append((idx, dt.month-1, dt.day-1))\n",
    "    return _processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f793a8c-de55-4c12-8dd2-c3e6fa1f9e60",
   "metadata": {},
   "source": [
    "## Create birthday data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce4eaea4-daf0-4996-96c9-0826a537b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_cpus = os.cpu_count()\n",
    "\n",
    "class BirthdayDataModule(LightningDataModule):\n",
    "    train_dataset: List[Tuple[int, int]]\n",
    "    val_dataset: List[Tuple[int, int]]\n",
    "    batch_size: int\n",
    "    def __init__(self, dm: WikipediaDataModule, batch_size: int = 64):\n",
    "        super().__init__()\n",
    "        self.train_dataset = process_dataset(dm.train_dataset)\n",
    "        self.val_dataset = process_dataset(dm.val_dataset)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = min(4, num_cpus)\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        return\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False # Only shuffle for train\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13aa79a4-cced-4f94-a39b-1534759fd5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ba5ed168ed4cc8b04111d9c6ed62e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing birthdays:   0%|          | 0/582659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61f75331dda4aa29d7fdcdb38623a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing birthdays:   0%|          | 0/14566 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "birthday_dm = BirthdayDataModule(dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2982e8b-786a-4e7e-a0c8-957ecbef6e29",
   "metadata": {},
   "source": [
    "## Create birthday model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07850e58-4195-4b64-bc1c-899d3df4820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchmetrics\n",
    "import transformers\n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "from transformers import AdamW\n",
    "\n",
    "class BirthdayModel(LightningModule):\n",
    "    \"\"\"Probes the PROFILE for birthday info.\"\"\"\n",
    "    model: DocumentProfileMatchingTransformer\n",
    "    \n",
    "    train_profiles: np.array\n",
    "    val_profiles: np.array\n",
    "    \n",
    "    classifier: torch.nn.Module\n",
    "    learning_rate: float\n",
    "    \n",
    "    def __init__(self, model: DocumentProfileMatchingTransformer, dm: WikipediaDataModule, learning_rate: float):\n",
    "        super().__init__()\n",
    "        # We can pre-calculate these embeddings bc\n",
    "        self.model = model\n",
    "        self.train_profiles = np.array(dm.train_dataset['profile'])\n",
    "        self.val_profiles = np.array(dm.val_dataset['profile'])\n",
    "        self.month_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(model.profile_embedding_dim, 64),\n",
    "            # torch.nn.Dropout(p=0.01),\n",
    "            torch.nn.Linear(64, 12),\n",
    "        )\n",
    "        self.day_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(model.profile_embedding_dim, 64),\n",
    "            # torch.nn.Dropout(p=0.01),\n",
    "            torch.nn.Linear(64, 31),\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_accuracy = torchmetrics.Accuracy()\n",
    "        self.val_accuracy   = torchmetrics.Accuracy()\n",
    "        self.loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def training_step(self, batch: Tuple[int, int], batch_idx: int) -> torch.Tensor:\n",
    "        profile_idxs, months, days = batch\n",
    "        assert ((0 <= profile_idxs) & (profile_idxs < len(self.train_profiles))).all()\n",
    "        assert ((0 <= months) & (months < 12)).all()\n",
    "        assert ((0 <= days) & (days < 31)).all()\n",
    "        \n",
    "        profiles = self.train_profiles[profile_idxs.cpu()].tolist()\n",
    "        embedding = self.model.forward_profile_text(text=profiles)\n",
    "        \n",
    "        month_logits = self.month_classifier(embedding)\n",
    "        day_logits = self.day_classifier(embedding)\n",
    "        \n",
    "        \n",
    "        month_loss = torch.nn.functional.cross_entropy(month_logits, months)\n",
    "        day_loss = torch.nn.functional.cross_entropy(day_logits, days)\n",
    "        \n",
    "        self.log('train_acc_month', self.train_accuracy(month_logits, months))\n",
    "        self.log('train_acc_day', self.train_accuracy(day_logits, days))\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                'train_acc_month', self.train_accuracy(month_logits, months), \n",
    "                '; train_acc_day', self.train_accuracy(day_logits, days)\n",
    "            )\n",
    "        \n",
    "        return (month_loss + day_loss)\n",
    "    \n",
    "    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int):\n",
    "        profile_idxs, months, days = batch\n",
    "        assert ((0 <= profile_idxs) & (profile_idxs < len(self.val_profiles))).all()\n",
    "        assert ((0 <= months) & (months < 12)).all()\n",
    "        assert ((0 <= days) & (days < 31)).all()\n",
    "        \n",
    "        profiles = self.val_profiles[profile_idxs.cpu()].tolist()\n",
    "        embedding = self.model.forward_profile_text(text=profiles)\n",
    "        \n",
    "        month_logits = self.month_classifier(embedding)\n",
    "        day_logits = self.day_classifier(embedding)\n",
    "        \n",
    "        \n",
    "        month_loss = torch.nn.functional.cross_entropy(month_logits, months)\n",
    "        day_loss = torch.nn.functional.cross_entropy(day_logits, days)\n",
    "        \n",
    "        self.log('val_acc_month', self.val_accuracy(month_logits, months))\n",
    "        self.log('val_acc_day', self.val_accuracy(day_logits, days))\n",
    "        \n",
    "        if (batch_idx == 0) or (batch_idx == 1855):\n",
    "            print(\n",
    "                batch_idx,\n",
    "                'val_acc_month', self.val_accuracy(month_logits, months),\n",
    "                '; val_acc_day', self.val_accuracy(day_logits, days)\n",
    "            )\n",
    "\n",
    "        return (month_loss + day_loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        params = (\n",
    "            list(self.day_classifier.parameters()) + \n",
    "            list(self.month_classifier.parameters()) +\n",
    "            list(self.model.profile_model.parameters())\n",
    "        )\n",
    "        optimizer = AdamW(\n",
    "            params, lr=self.learning_rate\n",
    "        )\n",
    "        return optimizer\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e8da2e-b0c4-417e-8a39-34e2b20fdbb4",
   "metadata": {},
   "source": [
    "## Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99ae16d1-a388-4352-8e46-ef86bc8232ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer, seed_everything\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "num_validations_per_epoch = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9ca3e84-33e7-4cde-a4ad-a12334cbcb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "birthday_model = BirthdayModel(model=model, dm=dm, learning_rate=5e-5)\n",
    "birthday_dm.batch_size = 128\n",
    "\n",
    "val_per_epoch = 4\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "trainer = Trainer(\n",
    "    default_root_dir=f\"saves/jup/birthday_probing\",\n",
    "    val_check_interval=1/val_per_epoch,\n",
    "    max_epochs=5,\n",
    "    log_every_n_steps=50,\n",
    "    gpus=torch.cuda.device_count(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad30bce-e533-47d8-9cd0-756bd000973d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jxm3/.conda/envs/textattack/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/jxm3/.conda/envs/textattack/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name             | Type                               | Params\n",
      "------------------------------------------------------------------------\n",
      "0 | model            | DocumentProfileMatchingTransformer | 67.0 M\n",
      "1 | month_classifier | Sequential                         | 50.0 K\n",
      "2 | day_classifier   | Sequential                         | 51.2 K\n",
      "3 | train_accuracy   | Accuracy                           | 0     \n",
      "4 | val_accuracy     | Accuracy                           | 0     \n",
      "5 | loss_criterion   | CrossEntropyLoss                   | 0     \n",
      "------------------------------------------------------------------------\n",
      "67.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "67.1 M    Total params\n",
      "268.219   Total estimated model params size (MB)\n",
      "/home/jxm3/.conda/envs/textattack/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory saves/jup/birthday_probing/lightning_logs/version_525170/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 val_acc_month tensor(0.0859, device='cuda:0') ; val_acc_day tensor(0.0391, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16fede9d4b44aa9a7d38f90268bfbd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc_month tensor(0.0938, device='cuda:0') ; train_acc_day tensor(0.0547, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 val_acc_month tensor(0.9922, device='cuda:0') ; val_acc_day tensor(1., device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 val_acc_month tensor(0.9922, device='cuda:0') ; val_acc_day tensor(0.9922, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(birthday_model, birthday_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d3ff83-af59-4cf8-80a6-554aade5ebec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
